# å¼ºåŒ–å­¦ä¹ æ¼”è¿›ï¼šä»åŸºç¡€ç†è®ºåˆ°ç”Ÿæˆå¼æ¨è

> **ä½œè€…**: konyellin-cyber  
> **åˆ›å»ºæ—¶é—´**: 2026-02-16  
> **æ–‡æ¡£ç›®æ ‡**: ç†è§£å¼ºåŒ–å­¦ä¹ åœ¨æ¨èç³»ç»Ÿä¸­çš„æ¼”è¿›è·¯å¾„ï¼ŒæŒæ¡ä¸ºä»€ä¹ˆç°ä»£æ¨èç³»ç»Ÿé€‰æ‹©ç­–ç•¥æ¢¯åº¦è€Œéä»·å€¼æ–¹æ³•  
> **å‰ç½®çŸ¥è¯†**: ç¬¬1-3ç« ï¼ˆåŸºç¡€æ¦‚å¿µã€MDPã€è´å°”æ›¼æ–¹ç¨‹ï¼‰

---

## ç›®å½•

1. [å¼•è¨€ï¼šä¸ºä»€ä¹ˆè¦å­¦ä¹ è¿™ä¸€ç« ï¼Ÿ](#1-å¼•è¨€ä¸ºä»€ä¹ˆè¦å­¦ä¹ è¿™ä¸€ç« )
2. [é—®é¢˜çš„æå‡ºï¼šæ¨èç³»ç»Ÿé‡åˆ°äº†ä»€ä¹ˆå›°å¢ƒï¼Ÿ](#2-é—®é¢˜çš„æå‡ºæ¨èç³»ç»Ÿé‡åˆ°äº†ä»€ä¹ˆå›°å¢ƒ)
3. [ä¸¤æ¡æŠ€æœ¯è·¯å¾„çš„åˆ†åŒ–](#3-ä¸¤æ¡æŠ€æœ¯è·¯å¾„çš„åˆ†åŒ–)
4. [Value-Based æ–¹æ³•ï¼šä¸ºä»€ä¹ˆä¸é€‚ç”¨ï¼Ÿ](#4-value-based-æ–¹æ³•ä¸ºä»€ä¹ˆä¸é€‚ç”¨)
5. [Policy-Based æ–¹æ³•ï¼šä¸ºä»€ä¹ˆæˆä¸ºä¸»æµï¼Ÿ](#5-policy-based-æ–¹æ³•ä¸ºä»€ä¹ˆæˆä¸ºä¸»æµ)
6. [ç”Ÿæˆå¼æ¶æ„çš„èåˆ](#6-ç”Ÿæˆå¼æ¶æ„çš„èåˆ)
7. [å®è·µæŒ‡å—ï¼šå¦‚ä½•é€‰æ‹©ç®—æ³•ï¼Ÿ](#7-å®è·µæŒ‡å—å¦‚ä½•é€‰æ‹©ç®—æ³•)
8. [å­¦ä¹ è·¯å¾„å»ºè®®](#8-å­¦ä¹ è·¯å¾„å»ºè®®)
9. [æ€»ç»“ä¸å…³é”®æ´å¯Ÿ](#9-æ€»ç»“ä¸å…³é”®æ´å¯Ÿ)
10. [é™„å½•ï¼šå®Œæ•´æ¼”è¿›å›¾è°±](#10-é™„å½•å®Œæ•´æ¼”è¿›å›¾è°±)

---

## 1. å¼•è¨€ï¼šä¸ºä»€ä¹ˆè¦å­¦ä¹ è¿™ä¸€ç« ï¼Ÿ

### 1.1 å‰ä¸‰ç« å­¦äº†ä»€ä¹ˆï¼Ÿ

åœ¨å‰ä¸‰ç« ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿå­¦ä¹ äº†å¼ºåŒ–å­¦ä¹ çš„ç†è®ºåŸºç¡€ï¼š

- **ç¬¬1ç« **ï¼šå¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µï¼ˆAgentã€Environmentã€Rewardã€Policyï¼‰
- **ç¬¬2ç« **ï¼šé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å¦‚ä½•å»ºæ¨¡æ¨èé—®é¢˜
- **ç¬¬3ç« **ï¼šè´å°”æ›¼æ–¹ç¨‹å¦‚ä½•å»ºç«‹ä»·å€¼å‡½æ•°çš„é€’å½’å…³ç³»

### 1.2 æœ¬ç« è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ

å­¦å®Œå‰ä¸‰ç« ï¼Œä½ å¯èƒ½ä¼šæœ‰è¿™äº›ç–‘æƒ‘ï¼š

â“ **ç–‘æƒ‘1**ï¼šè´å°”æ›¼æ–¹ç¨‹æ¨å¯¼å‡ºäº† Q-Learning å’Œ DQNï¼Œä¸ºä»€ä¹ˆå¿«æ‰‹ OneRec ä¸ç”¨å®ƒä»¬ï¼Ÿ  
â“ **ç–‘æƒ‘2**ï¼šæ—¢ç„¶ä»·å€¼æ–¹æ³•ï¼ˆQ-Learningï¼‰æ˜¯ç»å…¸ç®—æ³•ï¼Œä¸ºä»€ä¹ˆç°ä»£æ¨èç³»ç»Ÿéƒ½é€‰ç­–ç•¥æ¢¯åº¦ï¼ˆPPO/ECPOï¼‰ï¼Ÿ  
â“ **ç–‘æƒ‘3**ï¼šç†è®ºåˆ°å®è·µçš„è·¨è¶Šåˆ°åº•æœ‰å¤šå¤§ï¼Ÿå­¦å®ŒåŸºç¡€ç†è®ºåè¯¥å¦‚ä½•è¿›é˜¶ï¼Ÿ

### 1.3 æœ¬ç« çš„å­¦ä¹ ç›®æ ‡

è¯»å®Œæœ¬ç« ï¼Œä½ å°†èƒ½å¤Ÿï¼š

âœ… **ç†è§£ç®—æ³•é€‰æ‹©çš„é€»è¾‘**ï¼šä¸ºä»€ä¹ˆä¸åŒåœºæ™¯é€‚åˆä¸åŒç®—æ³•  
âœ… **æŒæ¡ä¸¤å¤§æŠ€æœ¯è·¯å¾„**ï¼šValue-Based vs Policy-Based çš„æœ¬è´¨åŒºåˆ«  
âœ… **å»ºç«‹å®Œæ•´çŸ¥è¯†ä½“ç³»**ï¼šä»åŸºç¡€ç†è®ºåˆ°ç”Ÿæˆå¼æ¨èçš„æ¼”è¿›è„‰ç»œ  
âœ… **æŒ‡å¯¼å®è·µå­¦ä¹ **ï¼šçŸ¥é“ä¸‹ä¸€æ­¥è¯¥å­¦ä»€ä¹ˆã€æ€ä¹ˆå­¦

---

## 2. é—®é¢˜çš„æå‡ºï¼šæ¨èç³»ç»Ÿé‡åˆ°äº†ä»€ä¹ˆå›°å¢ƒï¼Ÿ

### 2.1 ä¼ ç»Ÿæ¨èç³»ç»Ÿçš„æ¶æ„

åœ¨è®¨è®ºå¼ºåŒ–å­¦ä¹ ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå›é¡¾ä¼ ç»Ÿæ¨èç³»ç»Ÿçš„åšæ³•ï¼š

```
ç”¨æˆ·è¯·æ±‚ â†’ å¬å› â†’ ç²—æ’ â†’ ç²¾æ’ â†’ é‡æ’ â†’ æ¨èåˆ—è¡¨
          (ä¸‡çº§)  (åƒçº§)  (ç™¾çº§)  (Kä¸ª)
```

**æ ¸å¿ƒé—®é¢˜**ï¼š
- âŒ **çº§è”æ¶æ„**ï¼šæ¯ä¸ªé˜¶æ®µç‹¬ç«‹ä¼˜åŒ–ï¼Œå±€éƒ¨æœ€ä¼˜ â‰  å…¨å±€æœ€ä¼˜
- âŒ **å•æ¬¡å†³ç­–**ï¼šæ¯æ¬¡åªè€ƒè™‘å½“å‰ç‚¹å‡»ç‡ï¼Œå¿½ç•¥é•¿æœŸç”¨æˆ·ç•™å­˜
- âŒ **å»¶è¿Ÿå¥–åŠ±éš¾å¤„ç†**ï¼šç”¨æˆ·æ˜¯çœ‹å®Œæ•´ä¸ªåˆ—è¡¨åæ‰äº§ç”Ÿè¡Œä¸ºï¼ˆç‚¹èµ/åˆ†äº«/ç•™å­˜ï¼‰

### 2.2 å¼ºåŒ–å­¦ä¹ çš„ç†æƒ³ä¸ç°å®

**ç†æƒ³**ï¼šç”¨å¼ºåŒ–å­¦ä¹ è§£å†³è¿™äº›é—®é¢˜ï¼
- å°†æ¨èå»ºæ¨¡ä¸º MDPï¼šçŠ¶æ€ = ç”¨æˆ·ç‰¹å¾ï¼ŒåŠ¨ä½œ = é€‰æ‹© itemï¼Œå¥–åŠ± = ç”¨æˆ·åé¦ˆ
- ç”¨è´å°”æ›¼æ–¹ç¨‹ä¼˜åŒ–é•¿æœŸä»·å€¼
- ç”¨ Q-Learning æˆ– DQN è®­ç»ƒç­–ç•¥

**ç°å®**ï¼šé‡åˆ°äº†å·¨å¤§çš„æŠ€æœ¯éšœç¢ï¼

### 2.3 æ¨èåœºæ™¯çš„ä¸‰å¤§æŒ‘æˆ˜

#### æŒ‘æˆ˜ 1ï¼šåŠ¨ä½œç©ºé—´çˆ†ç‚¸ ğŸ’¥

| åœºæ™¯ | åŠ¨ä½œç©ºé—´å¤§å° | Qè¡¨å¤§å° |
|------|-------------|---------|
| Atariæ¸¸æˆ | 18ä¸ªç¦»æ•£åŠ¨ä½œ | å¯æšä¸¾ |
| å›´æ£‹ | 19Ã—19=361ä¸ªä½ç½® | å¯å¤„ç† |
| **æ¨èç³»ç»Ÿ** | **ç™¾ä¸‡çº§ item æ± ** | **æ— æ³•æšä¸¾** |
| **ç”Ÿæˆå¼æ¨è** | **Kä¸ªä½ç½®çš„æ’åˆ—ç»„åˆ** | **10^18 é‡çº§** |

**å…·ä½“è®¡ç®—**ï¼š
- Itemæ± å¤§å°ï¼š1,000,000
- æ¨èåˆ—è¡¨é•¿åº¦ï¼šK=10
- ç»„åˆæ•°ï¼šP(10^6, 10) â‰ˆ 10^60ï¼ˆè€ƒè™‘é¡ºåºï¼‰

âŒ **Q-Learning/DQN çš„æ ¸å¿ƒæ“ä½œ**ï¼š
```python
best_action = argmax_a Q(s, a)  # éœ€è¦éå†æ‰€æœ‰åŠ¨ä½œï¼
```

åœ¨æ¨èç³»ç»Ÿä¸­ï¼Œè¿™æ„å‘³ç€ï¼š
```python
# ä¸å¯è¡Œçš„ä¼ªä»£ç 
best_item = argmax_{item in 1,000,000} Q(user_state, item)  
# æ¯æ¬¡å†³ç­–éœ€è¦è®¡ç®— 100ä¸‡æ¬¡ï¼
```

#### æŒ‘æˆ˜ 2ï¼šçŠ¶æ€ç©ºé—´è¿ç»­ä¸”é«˜ç»´ ğŸŒŒ

**ç”¨æˆ·çŠ¶æ€çš„ç»„æˆ**ï¼š
```python
state = {
    'user_id': ç”¨æˆ·IDï¼ˆé«˜ç»´embeddingï¼Œå¦‚512ç»´ï¼‰,
    'browsing_history': æœ€è¿‘100ä¸ªæµè§ˆitemçš„embedding,
    'context': {æ—¶é—´ã€ä½ç½®ã€è®¾å¤‡ã€ç½‘ç»œç­‰},
    'generated_items': å·²ç”Ÿæˆçš„æ¨èåˆ—è¡¨
}
```

**ç»´åº¦çˆ†ç‚¸**ï¼š
- User embedding: 512ç»´
- History: 100 Ã— 512 = 51,200ç»´
- Context: ~20ç»´
- **æ€»ç»´åº¦**: 50,000+ ç»´è¿ç»­ç©ºé—´

âŒ **ä¼ ç»Ÿ Q-Learning çš„å‡è®¾**ï¼š
- çŠ¶æ€å¯ç¦»æ•£åŒ–ï¼ˆå¦‚ç½‘æ ¼ä¸–ç•Œï¼‰
- Qè¡¨å¯ä»¥å­˜å‚¨ï¼š`Q[state][action] = value`

#### æŒ‘æˆ˜ 3ï¼šç”Ÿæˆä»»åŠ¡çš„å¤©ç„¶ç‰¹æ€§ ğŸ¯

**æ¨èåˆ—è¡¨ç”Ÿæˆè¿‡ç¨‹**ï¼š
```
ç”Ÿæˆ item_1 â†’ åŸºäº item_1 ç”Ÿæˆ item_2 â†’ åŸºäº item_1,2 ç”Ÿæˆ item_3 â†’ ...
```

è¿™æ˜¯ä¸€ä¸ª**è‡ªå›å½’ç”Ÿæˆä»»åŠ¡**ï¼ˆç±»ä¼¼è¯­è¨€æ¨¡å‹ç”Ÿæˆå¥å­ï¼‰ï¼š
```python
P(o_1, o_2, ..., o_K | user) = P(o_1|user) Ã— P(o_2|user,o_1) Ã— ... Ã— P(o_K|user,o_1,...,o_{K-1})
```

âœ… **ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¤©ç„¶æ”¯æŒ**ï¼š
```python
policy(o_i | history) â†’ ç›´æ¥è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
```

âŒ **Value-Based æ–¹æ³•ä¸æ“…é•¿**ï¼š
```python
Q(s, a) â†’ è¯„ä¼°å•ä¸ªåŠ¨ä½œçš„ä»·å€¼ï¼Œéš¾ä»¥å»ºæ¨¡åºåˆ—ä¾èµ–
```

### 2.4 å¿…é¡»åšå‡ºé€‰æ‹©

é¢å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæ¨èç³»ç»Ÿå¿…é¡»åœ¨ä¸¤æ¡æŠ€æœ¯è·¯å¾„ä¸­åšå‡ºé€‰æ‹©ï¼š

```mermaid
graph TD
    A[è´å°”æ›¼æ–¹ç¨‹] --> B{æŠ€æœ¯è·¯å¾„é€‰æ‹©}
    B -->|è·¯å¾„1| C[Value-Based<br/>Q-Learning/DQN]
    B -->|è·¯å¾„2| D[Policy-Based<br/>Policy Gradient]
    
    C --> E[è¯„ä¼°æ¯ä¸ªåŠ¨ä½œçš„ä»·å€¼<br/>ç„¶åé€‰æœ€å¤§çš„]
    D --> F[ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°<br/>è¾“å‡ºåŠ¨ä½œåˆ†å¸ƒ]
    
    E --> G[âŒ æ¨èç³»ç»Ÿé‡é˜»]
    F --> H[âœ… å¿«æ‰‹é€‰æ‹©æ­¤è·¯å¾„]
    
    style G fill:#ffcdd2
    style H fill:#c8e6c9
```

---

## 3. ä¸¤æ¡æŠ€æœ¯è·¯å¾„çš„åˆ†åŒ–

### 3.1 ä»è´å°”æ›¼æ–¹ç¨‹å‡ºå‘çš„ä¸¤ä¸ªæ–¹å‘

å›é¡¾ç¬¬3ç« å­¦ä¹ çš„è´å°”æ›¼æ–¹ç¨‹ï¼š

**çŠ¶æ€ä»·å€¼å‡½æ•°**ï¼š
```
V^Ï€(s) = Î£_a Ï€(a|s) Î£_s' P(s'|s,a)[R(s,a,s') + Î³V^Ï€(s')]
```

**åŠ¨ä½œä»·å€¼å‡½æ•°**ï¼š
```
Q^Ï€(s,a) = Î£_s' P(s'|s,a)[R(s,a,s') + Î³ Î£_a' Ï€(a'|s')Q^Ï€(s',a')]
```

ä»è¿™ä¸¤ä¸ªæ–¹ç¨‹å‡ºå‘ï¼Œè¯ç”Ÿäº†ä¸¤å¤§æŠ€æœ¯è·¯å¾„ï¼š

| ç»´åº¦ | Value-Based | Policy-Based |
|------|------------|--------------|
| **å…³æ³¨å¯¹è±¡** | ä»·å€¼å‡½æ•° Q(s,a) | ç­–ç•¥å‡½æ•° Ï€(a\|s) |
| **ä¼˜åŒ–ç›®æ ‡** | å­¦ä¹ æœ€ä¼˜Qå‡½æ•° | ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•° |
| **åŠ¨ä½œé€‰æ‹©** | argmax_a Q(s,a) | sample from Ï€_Î¸(a\|s) |
| **ä»£è¡¨ç®—æ³•** | Q-Learning, DQN | REINFORCE, PPO |
| **æ¨èç³»ç»Ÿ** | âŒ é‡åˆ°ç“¶é¢ˆ | âœ… æˆä¸ºä¸»æµ |

### 3.2 ä¸¤æ¡è·¯å¾„çš„æ•°å­¦æœ¬è´¨

#### Value-Based: é—´æ¥ä¼˜åŒ–ç­–ç•¥

```
æ­¥éª¤1: å­¦ä¹ ä»·å€¼å‡½æ•°
  Q(s,a) â† è´å°”æ›¼æ›´æ–°

æ­¥éª¤2: æå–ç­–ç•¥
  Ï€(s) = argmax_a Q(s,a)
```

**ä¼˜åŠ¿**ï¼š
- âœ… æœ‰æ˜ç¡®çš„ç†è®ºåŸºç¡€ï¼ˆè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼‰
- âœ… Off-Policyå­¦ä¹ ï¼ˆå¯ä»¥åˆ©ç”¨å†å²æ•°æ®ï¼‰
- âœ… ç¡®å®šæ€§ç­–ç•¥ï¼ˆå®¹æ˜“ç†è§£å’Œè°ƒè¯•ï¼‰

**åŠ£åŠ¿**ï¼š
- âŒ éœ€è¦argmaxæ“ä½œï¼ˆå¤§åŠ¨ä½œç©ºé—´ä¸å¯è¡Œï¼‰
- âŒ ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆéš¾ä»¥å¤„ç†è¿ç»­æˆ–ç»„åˆåŠ¨ä½œï¼‰
- âŒ ç­–ç•¥æ”¹è¿›æ˜¯éšå¼çš„ï¼ˆé€šè¿‡æ”¹è¿›Qå‡½æ•°ï¼‰

#### Policy-Based: ç›´æ¥ä¼˜åŒ–ç­–ç•¥

```
ç›´æ¥ä¼˜åŒ–: ç­–ç•¥å‚æ•°åŒ–
  Ï€_Î¸(a|s) â† æ¢¯åº¦ä¸Šå‡
  
ç›®æ ‡å‡½æ•°:
  J(Î¸) = E_{s~Ï^Ï€, a~Ï€_Î¸}[R(s,a)]
  
æ›´æ–°è§„åˆ™:
  Î¸ â† Î¸ + Î·Â·âˆ‡_Î¸ J(Î¸)
```

**ä¼˜åŠ¿**ï¼š
- âœ… ä¸éœ€è¦argmaxï¼ˆç›´æ¥è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼‰
- âœ… æ”¯æŒè¿ç»­åŠ¨ä½œç©ºé—´
- âœ… å¤©ç„¶æ”¯æŒéšæœºç­–ç•¥
- âœ… é€‚åˆé«˜ç»´åŠ¨ä½œç©ºé—´

**åŠ£åŠ¿**ï¼š
- âŒ æ–¹å·®å¤§ï¼ˆéœ€è¦æŠ€å·§é™ä½æ–¹å·®ï¼‰
- âŒ å¯èƒ½æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜
- âŒ æ ·æœ¬æ•ˆç‡ç›¸å¯¹è¾ƒä½

---

## 4. Value-Based æ–¹æ³•ï¼šä¸ºä»€ä¹ˆä¸é€‚ç”¨ï¼Ÿ

### 4.1 ç®—æ³•æ¼”è¿›å†å²

```mermaid
graph LR
    A[è´å°”æ›¼æ–¹ç¨‹<br/>1950s] --> B[Q-Learning<br/>1989]
    B --> C[DQN<br/>2013]
    C --> D[Double DQN<br/>2015]
    D --> E[Dueling DQN<br/>2016]
    E --> F[Rainbow<br/>2017]
    
    F -.ä¸ºä»€ä¹ˆåœæ­¢?.-> G["âŒ åŠ¨ä½œç©ºé—´çˆ†ç‚¸"]
    F -.ä¸ºä»€ä¹ˆåœæ­¢?.-> H["âŒ maxæ“ä½œä¸å¯è¡Œ"]
    F -.ä¸ºä»€ä¹ˆåœæ­¢?.-> I["âŒ ç”Ÿæˆä»»åŠ¡ä¸æ”¯æŒ"]
    
    style F fill:#ffcdd2,stroke:#c62828
    style G fill:#fff,stroke:#d32f2f,stroke-dasharray:5 5
    style H fill:#fff,stroke:#d32f2f,stroke-dasharray:5 5
    style I fill:#fff,stroke:#d32f2f,stroke-dasharray:5 5
```

### 4.2 å„é˜¶æ®µè¯¦è§£

#### é˜¶æ®µ 1ï¼šQ-Learning (1989)

**ç®—æ³•æ ¸å¿ƒ**ï¼š
```python
# æ›´æ–°è§„åˆ™
Q(s, a) â† Q(s, a) + Î±[r + Î³Â·max_a' Q(s', a') - Q(s, a)]
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… å°è§„æ¨¡ç¦»æ•£çŠ¶æ€ç©ºé—´ï¼ˆå¦‚ç½‘æ ¼ä¸–ç•Œï¼‰
- âœ… ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆå¦‚ä¸Šä¸‹å·¦å³ï¼‰
- âœ… çŠ¶æ€-åŠ¨ä½œå¯¹å¯æšä¸¾

**æ¨èç³»ç»Ÿçš„éšœç¢**ï¼š
```python
# ç†æƒ³æƒ…å†µï¼ˆå°è§„æ¨¡æ–°é—»æ¨èï¼‰
Q_table = np.zeros((num_users, num_items))  
# å‡è®¾ 1000ç”¨æˆ· Ã— 100æ–°é—» = 100,000 ä¸ªå€¼ï¼Œå¯ä»¥å­˜å‚¨ âœ…

# ç°å®æƒ…å†µï¼ˆç”Ÿæˆå¼æ¨èï¼‰
Q_table = np.zeros((âˆ, 10^6))  
# è¿ç»­ç”¨æˆ·çŠ¶æ€ Ã— ç™¾ä¸‡itemæ±  = æ— é™å¤§ï¼Œæ— æ³•å­˜å‚¨ï¼âŒ
```

#### é˜¶æ®µ 2ï¼šDQN (2013-2015)

**åˆ›æ–°ç‚¹**ï¼šç”¨æ·±åº¦ç¥ç»ç½‘ç»œé€¼è¿‘ Q å‡½æ•°

```python
# ä¸å†å­˜å‚¨Qè¡¨ï¼Œè€Œæ˜¯è®­ç»ƒç¥ç»ç½‘ç»œ
Q(s, a; Î¸) â‰ˆ Q*(s, a)

# ç½‘ç»œç»“æ„
state (512ç»´) â†’ [Dense Layers] â†’ Q-values for all actions (|A|ç»´)
```

**å…³é”®æŠ€æœ¯**ï¼š
1. **Experience Replay**ï¼šæ‰“ç ´æ ·æœ¬ç›¸å…³æ€§
   ```python
   replay_buffer = [(s,a,r,s'), ...]
   batch = random_sample(replay_buffer)  # éšæœºé‡‡æ ·ï¼Œæ‰“ç ´æ—¶åºç›¸å…³
   ```

2. **Target Network**ï¼šç¨³å®šè®­ç»ƒ
   ```python
   target = r + Î³Â·max_a' Q_target(s', a')  # ç”¨å›ºå®šç½‘ç»œè®¡ç®—ç›®æ ‡
   loss = (target - Q(s,a))^2
   ```

**åœ¨Atariæ¸¸æˆçš„æˆåŠŸ**ï¼š
```python
state = æ¸¸æˆç”»é¢ (84Ã—84Ã—4)
actions = [ä¸Š, ä¸‹, å·¦, å³, å¼€ç«, ...]  # 18ä¸ªç¦»æ•£åŠ¨ä½œ
Q_network(state) â†’ [Q_1, Q_2, ..., Q_18]  # è¾“å‡º18ä¸ªQå€¼
best_action = argmax(Q_values)  # å¯è¡Œï¼âœ…
```

**åœ¨æ¨èç³»ç»Ÿçš„å¤±è´¥**ï¼š
```python
state = user_features (51,200ç»´)
actions = item_pool  # 1,000,000ä¸ªå€™é€‰item

# æ–¹æ¡ˆ1ï¼šè¾“å‡ºæ‰€æœ‰itemçš„Qå€¼ âŒ
Q_network(state) â†’ [Q_1, Q_2, ..., Q_1000000]  
# é—®é¢˜ï¼šç½‘ç»œè¾“å‡ºå±‚å¤ªå¤§ï¼ˆ100ä¸‡ç»´ï¼‰ï¼Œæ— æ³•è®­ç»ƒ

# æ–¹æ¡ˆ2ï¼šè¾“å…¥ (state, action) è¾“å‡ºå•ä¸ªQå€¼ âŒ
Q_network(concat(state, action)) â†’ Q_value
# é—®é¢˜ï¼šargmaxéœ€è¦å‰å‘ä¼ æ’­100ä¸‡æ¬¡ï¼Œå¤ªæ…¢ï¼

# æ–¹æ¡ˆ3ï¼šè¿‘ä¼¼æœç´¢ âŒ  
top_k = approximate_argmax(Q_values, k=100)
# é—®é¢˜ï¼šæ— æ³•ä¿è¯æ‰¾åˆ°çœŸæ­£çš„æœ€ä¼˜åŠ¨ä½œï¼Œæ¨èè´¨é‡ä¸‹é™
```

#### é˜¶æ®µ 3ï¼šRainbow DQN (2017)

**å¤šé¡¹æ”¹è¿›èåˆ**ï¼š
1. **Double DQN**ï¼šç¼“è§£Qå€¼è¿‡ä¼°è®¡
2. **Dueling DQN**ï¼šåˆ†ç¦»çŠ¶æ€ä»·å€¼V(s)å’Œä¼˜åŠ¿å‡½æ•°A(s,a)
3. **Prioritized Replay**ï¼šé‡è¦æ ·æœ¬ä¼˜å…ˆå­¦ä¹ 
4. **Noisy Nets**ï¼šå‚æ•°ç©ºé—´æ¢ç´¢
5. **Multi-step Learning**ï¼šn-step TD
6. **Distributional RL**ï¼šå­¦ä¹ ä»·å€¼åˆ†å¸ƒè€ŒéæœŸæœ›

**å·…å³°è¡¨ç°**ï¼š
- åœ¨57ä¸ªAtariæ¸¸æˆä¸­è¶…è¶Šäººç±»æ°´å¹³
- æˆä¸ºValue-Basedæ–¹æ³•çš„é›†å¤§æˆè€…

**ä½†ä»ç„¶æ— æ³•è§£å†³æ¨èç³»ç»Ÿçš„æ ¹æœ¬é—®é¢˜**ï¼š
- âŒ åŠ¨ä½œç©ºé—´çˆ†ç‚¸
- âŒ maxæ“ä½œè®¡ç®—ä¸å¯è¡Œ
- âŒ ç»„åˆç”Ÿæˆä»»åŠ¡ä¸æ”¯æŒ

### 4.3 ä¸‰ä¸ªæ ¹æœ¬éšœç¢

#### éšœç¢ 1ï¼šargmax æ“ä½œçš„è®¡ç®—å¤æ‚åº¦

**æ¯æ¬¡å†³ç­–çš„æ ¸å¿ƒæ“ä½œ**ï¼š
```python
best_action = argmax_a Q(s, a)
```

**è®¡ç®—æˆæœ¬å¯¹æ¯”**ï¼š

| åœºæ™¯ | åŠ¨ä½œæ•° | å•æ¬¡maxæˆæœ¬ | ç”ŸæˆK=10çš„æ€»æˆæœ¬ | å¯è¡Œæ€§ |
|------|-------|------------|----------------|--------|
| ç½‘æ ¼ä¸–ç•Œ | 4 | O(4) | O(40) | âœ… |
| Atari | 18 | O(18) | O(180) | âœ… |
| æ¨èç³»ç»Ÿ | 10^6 | O(10^6) | O(10^7) | âŒ |
| ç”Ÿæˆå¼æ¨è | 10^60 | O(10^60) | O(10^61) | âŒ |

**å³ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•**ï¼š
```python
# æ–¹æ³•1ï¼šåªè€ƒè™‘å¬å›çš„top-K
candidates = recall_top_k(user, k=1000)  # ä»100ä¸‡ç¼©å°åˆ°1000
Q_values = Q_network(state, candidates)
best = argmax(Q_values)  # O(1000)ï¼Œå¯æ¥å—

# ä½†å¸¦æ¥æ–°é—®é¢˜ï¼š
# 1. å¬å›é˜¶æ®µå¯èƒ½æ¼æ‰æœ€ä¼˜item
# 2. å¬å›å’Œæ’åºä¸¤é˜¶æ®µä¼˜åŒ–ä¸ä¸€è‡´
# 3. æ•´ä½“æ€§èƒ½å—å¬å›è´¨é‡é™åˆ¶
```

#### éšœç¢ 2ï¼šQå‡½æ•°çš„è¡¨ç¤ºèƒ½åŠ›

**Qå‡½æ•°éœ€è¦å­¦ä¹ çš„æ˜ å°„**ï¼š
```python
Q: (user_state, item) â†’ scalar_value

# éœ€è¦å­¦ä¹ çš„ç»„åˆæ•°
num_mappings = |States| Ã— |Actions|
             = âˆ Ã— 10^6  # è¿ç»­çŠ¶æ€ç©ºé—´ Ã— ç™¾ä¸‡item
             = âˆ
```

**æ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜**ï¼š
```python
# è®­ç»ƒæ—¶è§è¿‡çš„æ ·æœ¬
training_samples = {
    (user_1, item_100): 0.8,
    (user_1, item_205): 0.3,
    ...
}  # å‡è®¾æœ‰10äº¿æ¡äº¤äº’è®°å½•

# ä½†éœ€è¦é¢„æµ‹çš„ç»„åˆ
all_combinations = |Users| Ã— |Items|
                 = 10^9 Ã— 10^6
                 = 10^15

# æ ·æœ¬è¦†ç›–ç‡
coverage = 10^9 / 10^15 = 10^-6 = 0.0001%
```

**æ–°itemçš„å†·å¯åŠ¨**ï¼š
```python
# æ–°itemä¸Šçº¿
new_item = Item(id=1000001)

# Qç½‘ç»œå¦‚ä½•é¢„æµ‹ï¼Ÿ
Q(user, new_item) = ?  # è®­ç»ƒæ—¶ä»æœªè§è¿‡è¿™ä¸ªitem

# Policy-Basedæ–¹æ³•çš„ä¼˜åŠ¿
policy(new_item | user) âˆ exp(user_emb Â· item_emb / Ï„)
# å³ä½¿æ˜¯æ–°itemï¼Œåªè¦æœ‰embeddingå°±èƒ½é¢„æµ‹æ¦‚ç‡
```

#### éšœç¢ 3ï¼šåºåˆ—ç”Ÿæˆçš„ä¸é€‚é…

**æ¨èåˆ—è¡¨ç”Ÿæˆçš„æœ¬è´¨**ï¼š
```python
# ç›®æ ‡ï¼šç”ŸæˆKä¸ªitemçš„åºåˆ—
sequence = [item_1, item_2, ..., item_K]

# è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹
item_1 ~ P(Â·|user)
item_2 ~ P(Â·|user, item_1)
item_3 ~ P(Â·|user, item_1, item_2)
...
```

**Qå‡½æ•°çš„å»ºæ¨¡æ–¹å¼**ï¼š
```python
# Qå‡½æ•°è¯„ä¼°å•ä¸ªitemçš„ä»·å€¼
Q(state, item_i) = expected_future_reward

# ä½†æ— æ³•ç›´æ¥å»ºæ¨¡åºåˆ—ä¾èµ–ï¼š
# - item_2 çš„ä»·å€¼ä¾èµ–äº item_1 çš„é€‰æ‹©
# - éœ€è¦æ˜¾å¼ç»´æŠ¤"å·²é€‰æ‹©åˆ—è¡¨"ä½œä¸ºçŠ¶æ€çš„ä¸€éƒ¨åˆ†
# - çŠ¶æ€ç©ºé—´è¿›ä¸€æ­¥è†¨èƒ€
```

**Policy-Basedçš„å¤©ç„¶ä¼˜åŠ¿**ï¼š
```python
# ç­–ç•¥ç›´æ¥å»ºæ¨¡æ¡ä»¶æ¦‚ç‡
Ï€_Î¸(item_i | user, item_1, ..., item_{i-1})

# ç”¨Transformerç­‰åºåˆ—æ¨¡å‹å¤©ç„¶æ”¯æŒ
# Self-Attentionæœºåˆ¶è‡ªåŠ¨æ•æ‰åºåˆ—ä¾èµ–
```

### 4.4 Value-Based æ–¹æ³•çš„é€‚ç”¨è¾¹ç•Œ

å°½ç®¡åœ¨æ¨èç³»ç»Ÿé‡é˜»ï¼Œä½†Value-Basedæ–¹æ³•ä»ç„¶æœ‰å…¶ä»·å€¼ï¼š

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… å°è§„æ¨¡æ¨èï¼ˆitemæ±  < 1000ï¼‰
- âœ… ç¦»çº¿ç­–ç•¥è¯„ä¼°ï¼ˆæ— éœ€å®æ—¶argmaxï¼‰
- âœ… ä½œä¸ºå…¶ä»–ç®—æ³•çš„ç»„ä»¶ï¼ˆå¦‚Actor-Criticä¸­çš„Criticï¼‰
- âœ… ç†è®ºç ”ç©¶å’Œç®—æ³•åˆ†æ

**ä¸é€‚ç”¨åœºæ™¯**ï¼š
- âŒ å¤§è§„æ¨¡æ¨èï¼ˆitemæ±  > 10ä¸‡ï¼‰
- âŒ ç”Ÿæˆå¼æ¨èï¼ˆéœ€è¦é€æ­¥ç”Ÿæˆåºåˆ—ï¼‰
- âŒ å®æ—¶æ€§è¦æ±‚é«˜çš„åœ¨çº¿æ¨è

**å…³é”®æ•™è®­**ï¼š
> ç®—æ³•çš„é€‚ç”¨æ€§ä¸ä»…å–å†³äºç†è®ºçš„æ­£ç¡®æ€§ï¼Œæ›´å–å†³äºå®é™…åœºæ™¯çš„è®¡ç®—çº¦æŸå’Œä»»åŠ¡ç‰¹æ€§ã€‚

---

## 5. Policy-Based æ–¹æ³•ï¼šä¸ºä»€ä¹ˆæˆä¸ºä¸»æµï¼Ÿ

### 5.1 æ ¸å¿ƒæ€æƒ³

**åŸºæœ¬é€»è¾‘**ï¼š
1. **å‚æ•°åŒ–ç­–ç•¥**ï¼šÏ€_Î¸(a|s)ï¼Œç”¨ç¥ç»ç½‘ç»œè¡¨ç¤ºç­–ç•¥
2. **å®šä¹‰ç›®æ ‡**ï¼šJ(Î¸) = E[æ€»å›æŠ¥]
3. **æ¢¯åº¦ä¼˜åŒ–**ï¼šÎ¸ â† Î¸ + Î·Â·âˆ‡_Î¸ J(Î¸)

**ä¸Value-Basedçš„æœ¬è´¨åŒºåˆ«**ï¼š

```python
# Value-Based: é—´æ¥ç­–ç•¥ï¼ˆä¸¤æ­¥ï¼‰
Q(s, a) â†’ argmax_a Q(s, a) â†’ Ï€(s)
# å…ˆå­¦Qï¼Œå†æå–ç­–ç•¥

# Policy-Based: ç›´æ¥ç­–ç•¥ï¼ˆä¸€æ­¥ï¼‰
Î¸ â†’ Ï€_Î¸(a|s) â†’ sample action
# ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‚æ•°
```

### 5.1.1 æ·±å…¥ç†è§£ï¼šç­–ç•¥å­¦ä¹ çš„æœ¬è´¨

#### Policy-Based å­¦ä¹ çš„æ˜¯ä»€ä¹ˆï¼Ÿ

**ç›´æ¥å­¦ä¹ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ**ï¼š

Policy-Based æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ç›´æ¥å­¦ä¹ ä¸€ä¸ªç­–ç•¥å‡½æ•° Ï€_Î¸(a|s)ï¼Œå³ï¼š
> ç»™å®šå½“å‰çŠ¶æ€ sï¼Œè¾“å‡ºæ¯ä¸ªåŠ¨ä½œ a çš„æ¦‚ç‡åˆ†å¸ƒ

ä»¥æ¨èç³»ç»Ÿä¸ºä¾‹ï¼š
```python
policy(new_item | user) âˆ exp(user_emb Â· item_emb / Ï„)
```

**å…¬å¼è§£æ**ï¼š

| ç¬¦å· | å«ä¹‰ |
|------|------|
| `user_emb` | ç”¨æˆ·çš„å‘é‡è¡¨ç¤ºï¼ˆç¥ç»ç½‘ç»œå­¦ä¹ å¾—åˆ°ï¼‰ |
| `item_emb` | itemçš„å‘é‡è¡¨ç¤º |
| `user_emb Â· item_emb` | ç‚¹ç§¯ï¼Œè¡¨ç¤ºç”¨æˆ·ä¸itemçš„åŒ¹é…åº¦ |
| `Ï„` (temperature) | æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åˆ†å¸ƒçš„"å°–é”ç¨‹åº¦" |
| `exp(...)` | è½¬æ¢ä¸ºæ­£æ•°ï¼Œä¾¿äºå½’ä¸€åŒ–ä¸ºæ¦‚ç‡ |
| `âˆ` | æ­£æ¯”äºï¼ˆéœ€è¦é™¤ä»¥æ‰€æœ‰itemçš„æ€»å’Œæ¥å½’ä¸€åŒ–ï¼‰ |

**ç›´è§‚ç¤ºä¾‹**ï¼š
```python
# å‡è®¾æœ‰3ä¸ªå€™é€‰item
scores = [user_emb Â· item_1_emb,   # 0.8 (é«˜åŒ¹é…)
          user_emb Â· item_2_emb,   # 0.3 (ä¸­ç­‰åŒ¹é…)
          user_emb Â· item_3_emb]   # -0.2 (ä½åŒ¹é…)

# ç»è¿‡softmaxè½¬æ¢ä¸ºæ¦‚ç‡
probs = softmax(scores / Ï„)  # [0.6, 0.3, 0.1]

# ç­–ç•¥å°±æ˜¯è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒ
# 60%æ¦‚ç‡é€‰item_1ï¼Œ30%é€‰item_2ï¼Œ10%é€‰item_3
```

**ä¸ä¼ ç»Ÿæ–¹æ³•çš„æœ¬è´¨åŒºåˆ«**ï¼š

| ç»´åº¦ | Value-Based | Policy-Based |
|------|-------------|--------------|
| **å­¦ä¹ ç›®æ ‡** | Q(s, a) = "åœ¨çŠ¶æ€sä¸‹ï¼Œé€‰æ‹©åŠ¨ä½œaèƒ½è·å¾—å¤šå°‘ä»·å€¼" | Ï€(a\|s) = "åœ¨çŠ¶æ€sä¸‹ï¼Œé€‰æ‹©åŠ¨ä½œaçš„æ¦‚ç‡æ˜¯å¤šå°‘" |
| **å†³ç­–æ–¹å¼** | é€‰æ‹©ä»·å€¼æœ€å¤§çš„åŠ¨ä½œ â†’ argmax_a Q(s, a) | æŒ‰æ¦‚ç‡é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œ â†’ sample from Ï€(a\|s) |
| **è¾“å‡ºå†…å®¹** | ä¸€ä¸ªæ•°å€¼ï¼ˆä»·å€¼ï¼‰ | ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ |

#### ç­–ç•¥å­¦ä¹  vs CTRé¢„ä¼° vs è¯­è¨€æ¨¡å‹

å¾ˆå¤šäººä¼šç–‘æƒ‘ï¼šç­–ç•¥å­¦ä¹ è¾“å‡ºæ¦‚ç‡ï¼ŒCTRé¢„ä¼°ä¹Ÿè¾“å‡ºæ¦‚ç‡ï¼Œè¯­è¨€æ¨¡å‹ä¹Ÿè¾“å‡ºæ¦‚ç‡ï¼Œå®ƒä»¬æœ‰ä»€ä¹ˆæœ¬è´¨ä¸åŒï¼Ÿ

**1. æ ¸å¿ƒç›®æ ‡çš„å·®å¼‚**

| æ–¹æ³•ç±»å‹ | ç›®æ ‡ | å­¦ä¹ å†…å®¹ | ä¼˜åŒ–ç›®æ ‡ | ç‰¹ç‚¹ |
|---------|------|---------|---------|------|
| **CTRé¢„ä¼°** | P(ç‚¹å‡» \| user, item) | å•æ­¥é¢„æµ‹ - "è¿™ä¸ªç”¨æˆ·ä¼šä¸ä¼šç‚¹å‡»è¿™ä¸ªitem" | æœ€å°åŒ–é¢„æµ‹è¯¯å·®ï¼ˆBCEï¼‰ | ç›‘ç£å­¦ä¹ ï¼Œæœ‰çœŸå®æ ‡ç­¾ |
| **è¯­è¨€æ¨¡å‹** | P(ä¸‹ä¸€ä¸ªè¯ \| å‰æ–‡) | æ¡ä»¶ç”Ÿæˆ - "ç»™å®šå‰æ–‡ï¼Œä¸‹ä¸€ä¸ªè¯çš„åˆ†å¸ƒ" | æœ€å¤§åŒ–ä¼¼ç„¶ï¼ˆMLEï¼‰ | ç›‘ç£å­¦ä¹ ï¼Œè®­ç»ƒæ•°æ®å·²ç¡®å®š |
| **ç­–ç•¥å­¦ä¹ ** | Ï€(action \| state) | åºè´¯å†³ç­– - "ä»€ä¹ˆæƒ…å†µä¸‹åšä»€ä¹ˆèƒ½è·å¾—æœ€å¤§é•¿æœŸæ”¶ç›Š" | æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ± | å¼ºåŒ–å­¦ä¹ ï¼Œæ²¡æœ‰ç¡®å®šçš„"æ­£ç¡®ç­”æ¡ˆ" |

**2. è®­ç»ƒè¿‡ç¨‹çš„æœ¬è´¨å·®å¼‚**

**CTRé¢„ä¼°ï¼šç›‘ç£å­¦ä¹ **
```python
# ç¬¬1æ­¥ï¼šå·²ç»æœ‰å®Œæ•´çš„è®­ç»ƒæ•°æ®
train_data = load_historical_clicks()  # å†å²ç‚¹å‡»æ—¥å¿—

# ç¬¬2æ­¥ï¼šè®­ç»ƒæ¨¡å‹æ‹Ÿåˆæ•°æ®
for (user, item, label) in train_data:
    pred = model(user, item)
    loss = binary_cross_entropy(pred, label)
    loss.backward()
    
# ç‰¹ç‚¹ï¼šä¸€æ¬¡æ€§è®­ç»ƒï¼Œæ•°æ®æ˜¯é™æ€çš„ï¼Œæœ‰æ˜ç¡®çš„å¯¹é”™æ ‡ç­¾
```

**è¯­è¨€æ¨¡å‹ï¼šç›‘ç£å­¦ä¹ ï¼ˆè‡ªå›å½’ï¼‰**
```python
# ç¬¬1æ­¥ï¼šå·²ç»æœ‰å®Œæ•´çš„æ–‡æœ¬è¯­æ–™
corpus = load_text_data()  # ä¹¦ç±ã€ç½‘é¡µã€å¯¹è¯ç­‰

# ç¬¬2æ­¥ï¼šè®­ç»ƒæ¨¡å‹å­¦ä¹ ä¸‹ä¸€ä¸ªè¯åˆ†å¸ƒ
for sequence in corpus:
    for i in range(len(sequence)):
        context = sequence[:i]
        target = sequence[i]
        pred = model(context)
        loss = cross_entropy(pred, target)
        loss.backward()

# ç‰¹ç‚¹ï¼šæ¨¡ä»¿è®­ç»ƒæ•°æ®ä¸­çš„æ¨¡å¼ï¼Œç›®æ ‡æ˜¯å¤ç°çœŸå®æ–‡æœ¬
```

**ç­–ç•¥å­¦ä¹ ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆè¯•é”™å­¦ä¹ ï¼‰**
```python
# ç¬¬1æ­¥ï¼šæ²¡æœ‰è®­ç»ƒæ•°æ®ï¼éœ€è¦è¾¹äº¤äº’è¾¹å­¦ä¹ 
for episode in range(num_episodes):
    state = env.reset()  # åˆå§‹åŒ–ç”¨æˆ·çŠ¶æ€
    trajectory = []
    
    # ç¬¬2æ­¥ï¼šæ‰§è¡Œç­–ç•¥ï¼Œæ”¶é›†è½¨è¿¹
    for t in range(max_steps):
        # ç­–ç•¥è¾“å‡ºåŠ¨ä½œæ¦‚ç‡ï¼ˆä¸æ˜¯æ ¹æ®å·²çŸ¥ç­”æ¡ˆï¼ï¼‰
        action_probs = policy(state)
        
        # æŒ‰æ¦‚ç‡é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰
        action = sample(action_probs)
        
        # æ‰§è¡ŒåŠ¨ä½œï¼Œè§‚å¯Ÿç»“æœ
        next_state, reward, done = env.step(action)
        
        trajectory.append((state, action, reward))
        state = next_state
        
        if done:
            break
    
    # ç¬¬3æ­¥ï¼šæ ¹æ®å®é™…è·å¾—çš„å¥–åŠ±æ›´æ–°ç­–ç•¥
    returns = compute_returns(trajectory)  # è®¡ç®—æ¯æ­¥çš„ç´¯ç§¯å›æŠ¥
    
    for (state, action, G) in zip(trajectory, returns):
        # å¦‚æœè¿™ä¸ªåŠ¨ä½œè·å¾—é«˜å›æŠ¥ï¼Œå¢å¤§å®ƒçš„æ¦‚ç‡
        loss = -log(policy(action | state)) * G
        loss.backward()

# ç‰¹ç‚¹ï¼š
# - æ²¡æœ‰"æ­£ç¡®ç­”æ¡ˆ"ï¼Œåªæœ‰"å¥½å"ï¼ˆrewardï¼‰
# - éœ€è¦ä¸æ–­å°è¯•ï¼ˆexplorationï¼‰
# - å½“å‰å†³ç­–å½±å“æœªæ¥çŠ¶æ€ï¼ˆlong-termï¼‰
```

**3. æ•°å­¦å½¢å¼å¯¹æ¯”**

| ç»´åº¦ | CTRé¢„ä¼° | è¯­è¨€æ¨¡å‹ | ç­–ç•¥å­¦ä¹  |
|------|---------|----------|----------|
| **ä¼˜åŒ–ç›®æ ‡** | min E[(Å· - y)Â²] | max Î£ log P(w_t\|w_<t) | max E[Î£ Î³^t r_t] |
| **æ¢¯åº¦æ¥æº** | æ ‡ç­¾è¯¯å·® | çœŸå®token | å®é™…reward |
| **æŸå¤±å‡½æ•°** | BCE/MSE | NLL | Policy Gradient |
| **è®­ç»ƒæ•°æ®** | (x, y) å¯¹ | æ–‡æœ¬åºåˆ— | äº¤äº’è½¨è¿¹ |
| **æ˜¯å¦éœ€è¦ç¯å¢ƒ** | âŒ ä¸éœ€è¦ | âŒ ä¸éœ€è¦ | âœ… éœ€è¦ï¼ |
| **æ¢ç´¢æœºåˆ¶** | âŒ æ—  | âœ… æ¸©åº¦é‡‡æ · | âœ… Îµ-greedy/ç†µæ­£åˆ™ |
| **é•¿æœŸè§„åˆ’** | âŒ å•æ­¥ | âœ… åºåˆ— | âœ… MDP |

**4. æ¨èåœºæ™¯çš„å¯¹æ¯”**

**CTRé¢„ä¼°çš„åšæ³•**ï¼š
```python
# å¯¹æ¯ä¸ªå€™é€‰è§†é¢‘ï¼Œé¢„æµ‹ç‚¹å‡»æ¦‚ç‡
scores = []
for item in candidate_items:
    p_click = ctr_model(user, item)  # é¢„æµ‹ï¼š"ç”¨æˆ·ä¼šç‚¹è¿™ä¸ªå—ï¼Ÿ"
    scores.append(p_click)

# é€‰Top-10ï¼ˆè´ªå¿ƒé€‰æ‹©ï¼‰
top_10 = sorted(candidate_items, key=lambda x: x.score, reverse=True)[:10]

# é—®é¢˜ï¼š
# - åªè€ƒè™‘å•ä¸ªitemçš„ç‚¹å‡»ç‡ï¼ˆå±€éƒ¨æœ€ä¼˜ï¼‰
# - ä¸è€ƒè™‘æ¨èåˆ—è¡¨çš„æ•´ä½“æ•ˆæœï¼ˆå¤šæ ·æ€§ã€åç»­ç•™å­˜ç­‰ï¼‰
# - ç¬¬1ä¸ªè§†é¢‘å’Œç¬¬10ä¸ªè§†é¢‘ç‹¬ç«‹é¢„æµ‹ï¼ˆå¿½ç•¥é¡ºåºå½±å“ï¼‰
```

**ç­–ç•¥å­¦ä¹ çš„åšæ³•**ï¼š
```python
# åˆå§‹çŠ¶æ€
state = {
    'user_profile': user_embedding,
    'context': time_of_day,
    'generated_so_far': []
}

# é€æ­¥ç”Ÿæˆæ¨èåˆ—è¡¨
for position in range(10):
    # ç­–ç•¥è¾“å‡ºåŠ¨ä½œæ¦‚ç‡ï¼ˆè€ƒè™‘é•¿æœŸå½±å“ï¼‰
    action_probs = policy_model(state)  # Ï€_Î¸(item | state)
    
    # é‡‡æ ·ï¼ˆå¸¦æ¢ç´¢ï¼‰
    selected_item = sample(action_probs)
    
    # æ›´æ–°çŠ¶æ€ï¼ˆåŒ…å«å·²æ¨èçš„itemï¼‰
    state['generated_so_far'].append(selected_item)

# ç”¨æˆ·è§‚çœ‹å®Œæ•´ä¸ªåˆ—è¡¨åï¼Œäº§ç”Ÿåé¦ˆ
reward = {
    'clicks': ç‚¹å‡»æ•°,
    'watch_time': æ€»è§‚çœ‹æ—¶é•¿,
    'session_length': ç”¨æˆ·ç»§ç»­æµè§ˆçš„æ—¶é—´
}

# æ ¹æ®rewardæ›´æ–°ç­–ç•¥
# å¦‚æœrewardé«˜ â†’ å¢å¤§è¿™10ä¸ªitemçš„é€‰æ‹©æ¦‚ç‡
# å¦‚æœrewardä½ â†’ å‡å°è¿™10ä¸ªitemçš„é€‰æ‹©æ¦‚ç‡

# ä¼˜ç‚¹ï¼š
# - è€ƒè™‘é•¿æœŸå½±å“ï¼ˆç”¨æˆ·ç•™å­˜ã€ä¸‹æ¬¡è¿˜ä¼šæ¥å—ï¼Ÿï¼‰
# - ç›´æ¥ä¼˜åŒ–ä¸šåŠ¡ç›®æ ‡ï¼ˆè§‚çœ‹æ—¶é•¿ã€ç•™å­˜ç‡ç­‰ï¼‰
# - è‡ªåŠ¨å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼ˆä¸ä¼šé™·å…¥æ¨èå•ä¸€ç±»å‹ï¼‰
```

**5. ä¸ºä»€ä¹ˆæ¨èç³»ç»Ÿéœ€è¦ç­–ç•¥å­¦ä¹ ï¼Ÿ**

CTRé¢„ä¼°çš„å±€é™ï¼š
```python
# å‡è®¾æœ‰3ä¸ªè§†é¢‘
video_A: CTR = 0.9 (æ ‡é¢˜å…šï¼Œç‚¹äº†ç«‹åˆ»å…³é—­)
video_B: CTR = 0.7 (ä¼˜è´¨å†…å®¹ï¼Œçœ‹å®Œè¿˜æƒ³çœ‹ä¸‹ä¸€ä¸ª)
video_C: CTR = 0.6 (å°ä¼—ä½†ç²¾å‡†)

# CTRæ¨¡å‹çš„å†³ç­–
recommendation = [A, Aç±»ä¼¼, Aç±»ä¼¼, ...]  # è´ªå¿ƒé€‰é«˜CTR

# ç»“æœï¼š
# - çŸ­æœŸæŒ‡æ ‡å¥½ï¼ˆç‚¹å‡»ç‡é«˜ï¼‰
# - é•¿æœŸæŒ‡æ ‡å·®ï¼ˆç”¨æˆ·åŒçƒ¦ï¼Œæµå¤±ï¼‰

# ç­–ç•¥å­¦ä¹ çš„å†³ç­–
# Ï€(video | user) è€ƒè™‘çš„æ˜¯ï¼š
# "æ¨èè¿™ä¸ªè§†é¢‘åï¼Œç”¨æˆ·çš„æ€»ä»·å€¼ï¼ˆLTVï¼‰ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿ"

# å¯èƒ½å­¦åˆ°çš„ç­–ç•¥ï¼š
# - ç¬¬1ä¸ªä½ç½®ï¼šæ”¾å¸å¼•çœ¼çƒçš„ï¼ˆä½†ä¸æ˜¯æ ‡é¢˜å…šï¼‰
# - ç¬¬2-5ä¸ªä½ç½®ï¼šæ”¾ç”¨æˆ·å¯èƒ½æ„Ÿå…´è¶£çš„ä¼˜è´¨å†…å®¹
# - ç¬¬6-8ä¸ªä½ç½®ï¼šæ¢ç´¢æ–°å†…å®¹ï¼ˆé¿å…ä¿¡æ¯èŒ§æˆ¿ï¼‰
# - ç¬¬9-10ä¸ªä½ç½®ï¼šæ”¾ç”¨æˆ·ç”»åƒç›¸å…³çš„é•¿å°¾å†…å®¹

# Rewardä¿¡å·ï¼š
reward = 0.1 * clicks + 0.3 * watch_time + 0.6 * user_retention
# è‡ªåŠ¨å­¦ä¼šå¹³è¡¡çŸ­æœŸç‚¹å‡»å’Œé•¿æœŸç•™å­˜ï¼
```

**æ ¸å¿ƒæ´å¯Ÿ**ï¼š

> **CTRé¢„ä¼°/è¯­è¨€æ¨¡å‹**ï¼šæ‹Ÿåˆå·²çŸ¥æ•°æ®ï¼ˆImitation Learningï¼‰  
> `Î¸* = argmin_Î¸ Distance(model_output, real_data)`
>
> **ç­–ç•¥å­¦ä¹ **ï¼šä¼˜åŒ–æœªæ¥å¥–åŠ±ï¼ˆReinforcement Learningï¼‰  
> `Î¸* = argmax_Î¸ E_future[cumulative_reward | policy_Î¸]`

**ç­–ç•¥å­¦ä¹ çš„æœ¬è´¨**ï¼š
- **å­¦ä¹ çš„æ˜¯è¡Œä¸ºæ¨¡å¼**ï¼šä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥æ€ä¹ˆåš
- **ä¼˜åŒ–çš„æ˜¯é•¿æœŸç›®æ ‡**ï¼šä¸æ˜¯é¢„æµ‹å‡†ç¡®ç‡ï¼Œè€Œæ˜¯ç´¯ç§¯å›æŠ¥
- **æ²¡æœ‰"æ­£ç¡®ç­”æ¡ˆ"**ï¼šåªæœ‰é€šè¿‡è¯•é”™æ‰¾åˆ°å¥½çš„ç­–ç•¥

### 5.2 ç®—æ³•æ¼”è¿›å†å²

```mermaid
graph TD
    A["ç­–ç•¥æ¢¯åº¦ç†è®º<br/>Williams 1992"] --> B["REINFORCE<br/>é«˜æ–¹å·®é—®é¢˜"]
    B --> C["Actor-Critic<br/>å¼•å…¥Baseline"]
    C --> D["A3C/A2C<br/>å¼‚æ­¥å¹¶è¡Œ 2016"]
    D --> E["TRPO<br/>Trust Region 2015"]
    E --> F["PPO<br/>Clipped Objective 2017"]
    F --> G["GRPO<br/>Group Relative 2024"]
    G --> H["ECPO<br/>Enhanced Clipping 2025"]
    
    I["âœ… é™ä½æ–¹å·®"] -.-> C
    J["âœ… ç¨³å®šè®­ç»ƒ"] -.-> E
    K["âœ… ç®€åŒ–å®ç°"] -.-> F
    L["âœ… æ¨èä¼˜åŒ–"] -.-> H
    
    style A fill:#e3f2fd
    style F fill:#fff3e0
    style H fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px
```

### 5.2.1 ç®—æ³•æ¼”è¿›åŠ¨æœº

æ¯ä¸ªç®—æ³•éƒ½åœ¨è§£å†³å‰ä¸€ä»£çš„å…·ä½“é—®é¢˜ï¼š
- **REINFORCE â†’ Actor-Critic**: é™ä½æ–¹å·®ï¼ˆå¼•å…¥Baselineï¼‰
- **Actor-Critic â†’ PPO**: æå‡ç¨³å®šæ€§ï¼ˆTrust Region/Clippingï¼‰
- **PPO â†’ GRPO**: é™ä½è®¡ç®—æˆæœ¬ï¼ˆå»æ‰Criticç½‘ç»œï¼‰
- **GRPO â†’ ECPO**: é’ˆå¯¹æ¨èåœºæ™¯ä¼˜åŒ–ï¼ˆè‡ªé€‚åº”Clippingï¼‰

### 5.3 å„é˜¶æ®µè¯¦è§£

#### é˜¶æ®µ 1ï¼šREINFORCE (1992)

**ç­–ç•¥æ¢¯åº¦å®šç†**ï¼š
```
âˆ‡_Î¸ J(Î¸) = E_{Ï„~Ï€_Î¸}[âˆ‘_t âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) Â· G_t]
```

**ç®—æ³•å®ç°**ï¼š
```python
# 1. ç”¨å½“å‰ç­–ç•¥é‡‡æ ·ä¸€ä¸ªè½¨è¿¹
trajectory = []
state = env.reset()
while not done:
    action = policy.sample(state)
    next_state, reward, done = env.step(action)
    trajectory.append((state, action, reward))
    state = next_state

# 2. è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥
G = []
g = 0
for t in reversed(range(len(trajectory))):
    g = trajectory[t][2] + gamma * g  # r_t + Î³Â·G_{t+1}
    G.insert(0, g)

# 3. è®¡ç®—ç­–ç•¥æ¢¯åº¦
for t in range(len(trajectory)):
    state, action, _ = trajectory[t]
    grad = grad_log_prob(policy, state, action) * G[t]
    theta += learning_rate * grad
```

**æ ¸å¿ƒé—®é¢˜ï¼šé«˜æ–¹å·®**
```python
# G_t çš„æ–¹å·®å¾ˆå¤§ï¼Œå¯¼è‡´æ¢¯åº¦ä¼°è®¡ä¸ç¨³å®š
# ä¾‹å¦‚ï¼šåŒæ ·çš„(s,a)ï¼Œä¸åŒè½¨è¿¹çš„Gå¯èƒ½å·®å¼‚å·¨å¤§
trajectory_1: G_t = 100
trajectory_2: G_t = -50
trajectory_3: G_t = 200
# å¹³å‡æ¢¯åº¦æ–¹å‘ä¸ç¨³å®š
```

#### é˜¶æ®µ 2ï¼šActor-Critic (å¼•å…¥Baseline)

**æ”¹è¿›æ€è·¯**ï¼šç”¨Advantageå‡½æ•°ä»£æ›¿åŸå§‹å›æŠ¥
```
A(s, a) = Q(s, a) - V(s)
```

**é™ä½æ–¹å·®çš„æ•°å­¦åŸç†**ï¼š
```
Var[G_t] > Var[G_t - V(s_t)]

# ç›´è§‚ç†è§£ï¼š
# G_tï¼šä»-100åˆ°+200ï¼ŒèŒƒå›´300
# G_t - V(s_t)ï¼šä»-10åˆ°+10ï¼ŒèŒƒå›´20ï¼ˆå‡è®¾Vä¼°è®¡å‡†ç¡®ï¼‰
```

**ç®—æ³•ç»“æ„**ï¼š
```python
# Actor: ç­–ç•¥ç½‘ç»œ
policy_net = PolicyNetwork(state_dim, action_dim)

# Critic: ä»·å€¼ç½‘ç»œ
value_net = ValueNetwork(state_dim)

# è®­ç»ƒæµç¨‹
state, action, reward, next_state = sample_transition()

# 1. è®¡ç®—Advantage
td_error = reward + gamma * value_net(next_state) - value_net(state)
advantage = td_error  # ç®€åŒ–ç‰ˆï¼Œå®é™…è¿˜å¯ä»¥ç”¨GAE

# 2. æ›´æ–°Actor
policy_loss = -log_prob(policy_net, state, action) * advantage
policy_net.update(policy_loss)

# 3. æ›´æ–°Critic
value_loss = (td_error)^2
value_net.update(value_loss)
```

**åœ¨æ¨èç³»ç»Ÿçš„åº”ç”¨**ï¼š
```python
# State: ç”¨æˆ·ç‰¹å¾ + å†å²è¡Œä¸º
state = concat(user_emb, history_emb, context)

# Actor: è¾“å‡ºitemåˆ†å¸ƒ
logits = actor_network(state)
probs = softmax(logits / temperature)

# Critic: è¯„ä¼°å½“å‰çŠ¶æ€ä»·å€¼
V = critic_network(state)

# ä¼˜åŠ¿ï¼š
# âœ… ä¸éœ€è¦æšä¸¾æ‰€æœ‰itemï¼ˆåªé‡‡æ ·å°‘é‡ï¼‰
# âœ… å¯ä»¥å¤„ç†ç™¾ä¸‡çº§itemæ± 
# âœ… æ”¯æŒç”Ÿæˆå¼ä»»åŠ¡
```

#### é˜¶æ®µ 3ï¼šTRPO (2015) - ç¨³å®šæ€§æå‡

**æ ¸å¿ƒåˆ›æ–°**ï¼šä¿¡ä»»åŸŸä¼˜åŒ–ï¼ˆTrust Region Policy Optimizationï¼‰

**é—®é¢˜**ï¼šç­–ç•¥æ›´æ–°æ­¥é•¿éš¾ä»¥æ§åˆ¶
```python
# ä¼ ç»Ÿæ¢¯åº¦æ›´æ–°
Î¸_new = Î¸_old + Î·Â·âˆ‡_Î¸ J(Î¸)

# é—®é¢˜ï¼š
# - Î·å¤ªå¤§ï¼šç­–ç•¥å´©æºƒï¼ˆperformance cliffï¼‰
# - Î·å¤ªå°ï¼šæ”¶æ•›æ…¢
# - Î·éšè®­ç»ƒé˜¶æ®µåŠ¨æ€å˜åŒ–ï¼Œéš¾ä»¥è°ƒå‚
```

**TRPOçš„è§£å†³æ–¹æ¡ˆ**ï¼š
```
max_Î¸ E[ratio(Î¸) Â· A]
s.t. KL(Ï€_Î¸_old || Ï€_Î¸) â‰¤ Î´

å…¶ä¸­:
ratio(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)
```

**ç›´è§‚è§£é‡Š**ï¼š
```
åœ¨"ä¿¡ä»»åŸŸ"å†…ä¼˜åŒ–ç­–ç•¥
  â†“
ç¡®ä¿æ–°ç­–ç•¥ä¸ä¼šåç¦»æ—§ç­–ç•¥å¤ªè¿œ
  â†“
ä¿è¯å•è°ƒæ”¹è¿›ï¼ˆç†è®ºä¿è¯ï¼‰
```

**åœ¨æ¨èç³»ç»Ÿçš„æ„ä¹‰**ï¼š
- âœ… åœ¨çº¿å­¦ä¹ çš„ç¨³å®šæ€§ï¼ˆç›´æ¥å½±å“ç”¨æˆ·ä½“éªŒï¼‰
- âœ… é¿å…ç­–ç•¥çªå˜ï¼ˆæ¨èç»“æœå‰§çƒˆå˜åŒ–ï¼‰
- âœ… ç†è®ºä¿è¯ï¼ˆæ€§èƒ½ä¸ä¼šä¸‹é™ï¼‰

**ä½†ä¹Ÿæœ‰é—®é¢˜**ï¼š
- âŒ å®ç°å¤æ‚ï¼ˆéœ€è¦è®¡ç®—äºŒé˜¶å¯¼æ•°ï¼‰
- âŒ è®¡ç®—æ˜‚è´µï¼ˆKLçº¦æŸçš„ä¼˜åŒ–ï¼‰

#### é˜¶æ®µ 4ï¼šPPO (2017) - OpenAIä¸»æ¨

**æ ¸å¿ƒåˆ›æ–°**ï¼šç”¨Clipped Objectiveç®€åŒ–TRPO

**ç®—æ³•å…¬å¼**ï¼š
```
L^CLIP(Î¸) = E[min(
    ratio(Î¸) Â· A,
    clip(ratio(Î¸), 1-Îµ, 1+Îµ) Â· A
)]

å…¶ä¸­:
ratio(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)
Îµ = 0.2 (å…¸å‹å€¼)
```

**ç›´è§‚ç†è§£**ï¼š
```python
# æƒ…å†µ1ï¼šAdvantage > 0ï¼ˆå¥½åŠ¨ä½œï¼Œå¸Œæœ›å¢åŠ æ¦‚ç‡ï¼‰
if ratio > 1+Îµ:  # æ–°ç­–ç•¥å·²ç»å¢åŠ å¾ˆå¤šäº†
    clip ratio to 1+Îµ  # é™åˆ¶å¢é•¿ï¼Œé˜²æ­¢è¿‡åº¦ä¼˜åŒ–

# æƒ…å†µ2ï¼šAdvantage < 0ï¼ˆååŠ¨ä½œï¼Œå¸Œæœ›å‡å°‘æ¦‚ç‡ï¼‰
if ratio < 1-Îµ:  # æ–°ç­–ç•¥å·²ç»å‡å°‘å¾ˆå¤šäº†
    clip ratio to 1-Îµ  # é™åˆ¶å‡å°‘ï¼Œé˜²æ­¢è¿‡åº¦æƒ©ç½š
```

**å¯è§†åŒ–Clippingæœºåˆ¶**ï¼š
```
Objective
    ^
    |     æœªè£å‰ªçº¿ï¼ˆå¯èƒ½ä¸ç¨³å®šï¼‰
    |    /
    |   /___  è£å‰ªåçš„å¹³å°
    |  /
    | /
    |/________è£å‰ªåçš„å¹³å°
    |         \
    +----------\---------> ratio
         1-Îµ  1  1+Îµ
```

**ç›¸æ¯”TRPOçš„ä¼˜åŠ¿**ï¼š
- âœ… å®ç°ç®€å•ï¼ˆä¸€é˜¶ä¼˜åŒ–ï¼Œæ— éœ€KLæ•£åº¦ï¼‰
- âœ… è®¡ç®—é«˜æ•ˆï¼ˆæ— éœ€äºŒé˜¶å¯¼æ•°ï¼‰
- âœ… æ•ˆæœcomparableï¼ˆå®éªŒè¡¨ç°æ¥è¿‘TRPOï¼‰
- âœ… è¶…å‚æ•°ç¨³å®šï¼ˆÎµ=0.2åœ¨å¤šæ•°ä»»åŠ¡è¡¨ç°è‰¯å¥½ï¼‰

**PPOæˆä¸ºä¸šç•Œæ ‡å‡†**ï¼š
- OpenAI ChatGPTçš„RLHFå°±ç”¨PPO
- DeepMindã€Googleã€Facebookéƒ½å¹¿æ³›é‡‡ç”¨
- æ¨èç³»ç»Ÿçš„é¦–é€‰ç­–ç•¥æ¢¯åº¦ç®—æ³•

**åœ¨æ¨èç³»ç»Ÿçš„åº”ç”¨**ï¼š
```python
# ä¼ªä»£ç 
class RecommendationPPO:
    def __init__(self):
        self.policy_net = TransformerPolicy()  # ç­–ç•¥ç½‘ç»œ
        self.value_net = ValueNetwork()  # ä»·å€¼ç½‘ç»œ
        
    def recommend(self, user_state):
        # ç”Ÿæˆæ¨èåˆ—è¡¨
        items = []
        for position in range(K):
            logits = self.policy_net(user_state, items)
            probs = softmax(logits)
            item = sample(probs)  # é‡‡æ ·è€Œéargmaxï¼
            items.append(item)
        return items
    
    def update(self, trajectories):
        # è®¡ç®—Advantage
        advantages = self.compute_gae(trajectories)
        
        # PPOæ›´æ–°
        for epoch in range(ppo_epochs):
            for batch in trajectories:
                # è®¡ç®—ratio
                old_probs = batch.old_log_probs
                new_probs = self.policy_net.log_prob(batch.states, batch.actions)
                ratio = exp(new_probs - old_probs)
                
                # Clipped Objective
                clip_ratio = clip(ratio, 1-epsilon, 1+epsilon)
                loss = -min(ratio * advantages, clip_ratio * advantages).mean()
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
```

#### é˜¶æ®µ 5ï¼šGRPO (2024) - DeepSeekåˆ›æ–°

**èƒŒæ™¯**ï¼šå¤§æ¨¡å‹å¾®è°ƒåœºæ™¯çš„æ–°éœ€æ±‚
- LLMç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆã€å¯¹è¯ï¼‰
- æ¨èç³»ç»Ÿçš„åºåˆ—ç”Ÿæˆ

**æ ¸å¿ƒåˆ›æ–°**ï¼šGroup Relative Policy Optimization
```
ä¸å†éœ€è¦Criticç½‘ç»œï¼ˆValue Functionï¼‰
ä½¿ç”¨ç»„å†…ç›¸å¯¹å¥–åŠ±

A_i = (r_i - Î¼_group) / Ïƒ_group

å…¶ä¸­ï¼š
- Î¼_group: åŒä¸€groupå†…æ‰€æœ‰æ ·æœ¬çš„å¹³å‡å¥–åŠ±
- Ïƒ_group: ç»„å†…å¥–åŠ±æ ‡å‡†å·®
```

**ç›´è§‚ç†è§£**ï¼š
```python
# ä¼ ç»ŸPPOï¼šéœ€è¦è®­ç»ƒCriticä¼°è®¡V(s)
advantage = Q(s,a) - V(s)
          = r + Î³V(s') - V(s)  # éœ€è¦Vç½‘ç»œ

# GRPOï¼šç”¨ç»„å†…æ¯”è¾ƒä»£æ›¿
group = [sample_1, sample_2, ..., sample_N]  # N=64
rewards = [r_1, r_2, ..., r_N]
advantage_i = (r_i - mean(rewards)) / std(rewards)
```

**ä¼˜åŠ¿**ï¼š
1. **æ— éœ€Criticç½‘ç»œ**
   - çœä¸€åŠå‚æ•°
   - çœä¸€åŠè®­ç»ƒæ—¶é—´
   - æ›´å®¹æ˜“éƒ¨ç½²

2. **æ›´ç¨³å®šçš„Advantageä¼°è®¡**
   ```python
   # PPOçš„Criticå¯èƒ½ä¼°è®¡ä¸å‡†
   V(s) = 5.0ï¼ˆçœŸå®åº”è¯¥æ˜¯10.0ï¼‰
   A = Q - V = 8.0 - 5.0 = 3.0ï¼ˆè¢«é«˜ä¼°ï¼‰
   
   # GRPOçš„ç»„å†…æ¯”è¾ƒæ›´é²æ£’
   rewards = [8.0, 7.5, 9.0, 8.2]
   A = (8.0 - 8.175) / 0.612 = -0.286ï¼ˆç›¸å¯¹åˆç†ï¼‰
   ```

3. **é€‚åˆå¤§è§„æ¨¡å¹¶è¡Œ**
   ```python
   # å¯ä»¥åŒæ—¶é‡‡æ ·å¤§æ‰¹é‡æ•°æ®
   batch = sample_batch(size=1024)
   split into groups of 64
   compute advantages within each group
   update policy
   ```

**åœ¨DeepSeek-R1çš„åº”ç”¨**ï¼š
- ç”¨äºæ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆä»»åŠ¡
- æ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡
- æˆä¸ºå¤§æ¨¡å‹RLHFçš„æ–°èŒƒå¼

**åœ¨æ¨èç³»ç»Ÿçš„æ½œåŠ›**ï¼š
- ç”Ÿæˆå¼æ¨èçš„åºåˆ—ä»»åŠ¡
- å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒ
- é™ä½æ¨¡å‹å¤æ‚åº¦

#### é˜¶æ®µ 6ï¼šECPO (2025) - å¿«æ‰‹OneRec

**èƒŒæ™¯**ï¼šå¿«æ‰‹åœ¨ç”Ÿæˆå¼æ¨èç³»ç»Ÿçš„å®è·µ

**åˆ›æ–°ç‚¹**ï¼šEnhanced Clipping Policy Optimization
```
èåˆPPOå’ŒGRPOçš„ä¼˜ç‚¹ï¼Œé’ˆå¯¹æ¨èåœºæ™¯ä¼˜åŒ–

L^ECPO(Î¸) = E[1/G âˆ‘_{i=1}^G min(
    ratio_i Â· A_i^group,
    enhanced_clip(ratio_i, A_i^group) Â· A_i^group
)]

å…¶ä¸­ï¼š
- G: ç”Ÿæˆçš„itemæ•°é‡
- A_i^group: GRPOé£æ ¼çš„ç»„ç›¸å¯¹Advantage
- enhanced_clip: é’ˆå¯¹æ¨èåœºæ™¯çš„è£å‰ªç­–ç•¥
```

**Enhanced Clippingçš„æ”¹è¿›**ï¼š
```python
# ä¼ ç»ŸPPOï¼šå›ºå®šè£å‰ªåŒºé—´
clip(ratio, 1-Îµ, 1+Îµ)

# ECPOï¼šè‡ªé€‚åº”è£å‰ª
def enhanced_clip(ratio, advantage):
    if advantage > 0:
        # å¥½åŠ¨ä½œï¼šæ ¹æ®ä¼˜åŠ¿å¤§å°åŠ¨æ€è°ƒæ•´ä¸Šç•Œ
        upper = 1 + Îµ * min(advantage / threshold, 1.0)
        return clip(ratio, 1, upper)
    else:
        # ååŠ¨ä½œï¼šæ ¹æ®åŠ£åŠ¿å¤§å°åŠ¨æ€è°ƒæ•´ä¸‹ç•Œ
        lower = 1 - Îµ * min(abs(advantage) / threshold, 1.0)
        return clip(ratio, lower, 1)
```

**åœ¨OneRecçš„å®Œæ•´æµç¨‹**ï¼š
```python
class OneRec:
    def __init__(self):
        # Encoder-Decoderæ¶æ„
        self.user_encoder = TransformerEncoder()
        self.item_decoder = TransformerDecoder()
        
        # è¯­ä¹‰ID
        self.item_tokenizer = RQKmeansTokenizer(vocab_size=100000)
    
    def recommend(self, user_features):
        # 1. ç¼–ç ç”¨æˆ·
        user_state = self.user_encoder(user_features)
        
        # 2. è‡ªå›å½’ç”Ÿæˆitemåºåˆ—
        items = []
        for pos in range(K):
            # ç”Ÿæˆè¯­ä¹‰IDï¼ˆè€Œéç›´æ¥ç”Ÿæˆitem IDï¼‰
            semantic_id = self.item_decoder.generate(
                user_state, items
            )
            items.append(semantic_id)
        
        # 3. è§£ç ä¸ºå®é™…item
        item_ids = self.item_tokenizer.decode(items)
        return item_ids
    
    def train_with_ecpo(self, user_batch):
        # 1. é‡‡æ ·å¤šä¸ªæ¨èåˆ—è¡¨ï¼ˆgroupï¼‰
        groups = []
        for _ in range(group_size):
            items = self.recommend(user_batch)
            reward = get_user_feedback(items)
            groups.append((items, reward))
        
        # 2. è®¡ç®—ç»„ç›¸å¯¹Advantage
        rewards = [r for _, r in groups]
        mean_r, std_r = np.mean(rewards), np.std(rewards)
        advantages = [(r - mean_r) / std_r for r in rewards]
        
        # 3. ECPOæ›´æ–°
        for (items, _), advantage in zip(groups, advantages):
            # è®¡ç®—ratioå’Œloss
            loss = self.ecpo_loss(items, advantage)
            loss.backward()
            optimizer.step()
```

**å¿«æ‰‹çš„å®é™…æ•ˆæœ**ï¼š
- ğŸ“ˆ ç‚¹å‡»ç‡æå‡ï¼š+X%
- ğŸ“ˆ è§‚çœ‹æ—¶é•¿æå‡ï¼š+Y%
- ğŸ“ˆ ç”¨æˆ·ç•™å­˜æå‡ï¼š+Z%
- âš¡ æ¨ç†å»¶è¿Ÿï¼š< 50msï¼ˆæ»¡è¶³çº¿ä¸Šè¦æ±‚ï¼‰

### 5.4 Policy-Based æ–¹æ³•çš„æ ¸å¿ƒä¼˜åŠ¿

#### ä¼˜åŠ¿ 1ï¼šæ— éœ€argmaxæ“ä½œ

```python
# Value-Based: éœ€è¦argmax
best_action = argmax_a Q(s, a)  # O(|A|)

# Policy-Based: ç›´æ¥é‡‡æ ·
action ~ Ï€_Î¸(Â·|s)  # O(1)
```

**åœ¨æ¨èç³»ç»Ÿçš„æ„ä¹‰**ï¼š
- âœ… å¯ä»¥å¤„ç†ç™¾ä¸‡çº§itemæ± 
- âœ… å®æ—¶æ¨ç†å»¶è¿Ÿå¯æ§
- âœ… æ”¯æŒTop-Ké‡‡æ ·ï¼ˆbeam searchï¼‰

#### ä¼˜åŠ¿ 2ï¼šå¤©ç„¶æ”¯æŒè¿ç»­å’Œç»„åˆç©ºé—´

```python
# è¿ç»­åŠ¨ä½œï¼ˆå¦‚è°ƒæ•´æ¨èæƒé‡ï¼‰
mean, std = policy_net(state)
action ~ Normal(mean, std)

# ç¦»æ•£åŠ¨ä½œï¼ˆé€‰æ‹©itemï¼‰
logits = policy_net(state)
action ~ Categorical(softmax(logits))

# ç»„åˆåŠ¨ä½œï¼ˆç”Ÿæˆåºåˆ—ï¼‰
for i in range(K):
    logits = policy_net(state, action[:i])
    action[i] ~ Categorical(softmax(logits))
```

#### ä¼˜åŠ¿ 3ï¼šéšæœºç­–ç•¥çš„æ¢ç´¢èƒ½åŠ›

```python
# Value-Based: ç¡®å®šæ€§ç­–ç•¥ï¼ˆéœ€è¦é¢å¤–æœºåˆ¶æ¢ç´¢ï¼‰
action = argmax_a Q(s, a)  # æ€»æ˜¯é€‰æœ€ä¼˜
# æˆ–
action = argmax_a Q(s, a) with Îµ-greedy  # æ‰‹åŠ¨åŠ æ¢ç´¢

# Policy-Based: è‡ªç„¶çš„éšæœºæ€§
action ~ Ï€_Î¸(Â·|s)  # æ ¹æ®æ¦‚ç‡é‡‡æ ·
# æ¢ç´¢-åˆ©ç”¨è‡ªåŠ¨å¹³è¡¡
```

**åœ¨æ¨èç³»ç»Ÿçš„æ„ä¹‰**ï¼š
- âœ… è‡ªç„¶çš„å¤šæ ·æ€§ï¼ˆä¸æ€»æ˜¯æ¨èåŒæ ·çš„å†…å®¹ï¼‰
- âœ… é¿å…ä¿¡æ¯èŒ§æˆ¿
- âœ… å†·å¯åŠ¨itemæœ‰æœºä¼šè¢«æ¨è

#### ä¼˜åŠ¿ 4ï¼šç«¯åˆ°ç«¯ä¼˜åŒ–

```python
# ä¼ ç»Ÿæ¨èï¼šçº§è”æ¶æ„
å¬å› â†’ ç²—æ’ â†’ ç²¾æ’ â†’ é‡æ’
# æ¯ä¸ªé˜¶æ®µç‹¬ç«‹ä¼˜åŒ–ï¼Œç›®æ ‡ä¸ä¸€è‡´

# ç”Ÿæˆå¼æ¨èï¼šç«¯åˆ°ç«¯
P(item_1, ..., item_K | user) = âˆ P(item_i | user, item_<i)
# å•ä¸€ç›®æ ‡ï¼šæœ€å¤§åŒ–ç”¨æˆ·æ»¡æ„åº¦
# Policy-Basedå¤©ç„¶æ”¯æŒè¿™ç§å»ºæ¨¡
```

### 5.5 ä»éœ€æ³¨æ„çš„æŒ‘æˆ˜

Policy-Basedæ–¹æ³•ä¹Ÿä¸æ˜¯å®Œç¾çš„ï¼š

**æŒ‘æˆ˜ 1ï¼šæ ·æœ¬æ•ˆç‡**
- éœ€è¦å¤§é‡åœ¨çº¿äº¤äº’æ•°æ®
- ç›¸æ¯”ç›‘ç£å­¦ä¹ ï¼Œæ ·æœ¬åˆ©ç”¨ç‡ä½
- è§£å†³ï¼šOffline RLã€æ¨¡æ‹Ÿå™¨

**æŒ‘æˆ˜ 2ï¼šè®­ç»ƒç¨³å®šæ€§**
- ç­–ç•¥æ¢¯åº¦æ–¹å·®å¤§
- è¶…å‚æ•°æ•æ„Ÿ
- è§£å†³ï¼šPPOã€TRPOã€GRPOç­‰æ”¹è¿›ç®—æ³•

**æŒ‘æˆ˜ 3ï¼šå±€éƒ¨æœ€ä¼˜**
- å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ç­–ç•¥
- è§£å†³ï¼šEntropy Regularizationã€å¤šæ¬¡é‡å¯

ä½†æ€»ä½“è€Œè¨€ï¼Œåœ¨å¤§è§„æ¨¡æ¨èç³»ç»Ÿä¸­ï¼š
> Policy-Based >> Value-Based

---

## 6. ç”Ÿæˆå¼æ¶æ„çš„èåˆ

### 6.1 Transformerçš„å¼•å…¥

**2017å¹´çš„çªç ´**ï¼šAttention is All You Need

**Transformeråœ¨æ¨èä¸­çš„åº”ç”¨å†ç¨‹**ï¼š

```mermaid
graph LR
    A[RNNæ¨è<br/>GRU4Rec 2016] --> B[Self-Attention<br/>SASRec 2018]
    B --> C[BERT4Rec<br/>2019]
    C --> D[ç”Ÿæˆå¼æ¨è<br/>2023+]
    
    E[Transformer<br/>2017] --> B
    E --> C
    E --> D
    
    style D fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px
```

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
1. **åºåˆ—å»ºæ¨¡èƒ½åŠ›**
   ```python
   # RNN: é¡ºåºå¤„ç†ï¼Œéš¾ä»¥å¹¶è¡Œ
   for t in range(T):
       h_t = RNN(h_{t-1}, x_t)
   
   # Transformer: å¹¶è¡Œå¤„ç†å…¨å±€ä¾èµ–
   H = SelfAttention(X)  # ä¸€æ¬¡å‰å‘ä¼ æ’­å¤„ç†æ•´ä¸ªåºåˆ—
   ```

2. **é•¿ç¨‹ä¾èµ–**
   ```python
   # RNN: æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
   # Transformer: ç›´æ¥å»ºæ¨¡ä»»æ„ä½ç½®çš„ä¾èµ–
   Attention(Q, K, V) = softmax(QK^T / âˆšd) V
   ```

3. **å¯æ‰©å±•æ€§**
   ```python
   # æ¨¡å‹å¤§å°å¯ä»¥ä»ç™¾ä¸‡åˆ°åƒäº¿å‚æ•°
   # æ•°æ®é‡å¯ä»¥ä»GBåˆ°PBçº§
   ```

### 6.2 Encoder-Decoderæ¶æ„åœ¨æ¨èä¸­çš„åº”ç”¨

**ä¼ ç»ŸSeq2Seq**ï¼š
```python
# Encoder: ç¼–ç è¾“å…¥åºåˆ—
encoder_output = Encoder(input_sequence)

# Decoder: ç”Ÿæˆè¾“å‡ºåºåˆ—
output_sequence = []
for t in range(T):
    y_t = Decoder(encoder_output, output_sequence)
    output_sequence.append(y_t)
```

**OneRecçš„æ¶æ„**ï¼š
```python
class OneRec(nn.Module):
    def __init__(self):
        # Encoder: ç¼–ç ç”¨æˆ·ç‰¹å¾
        self.user_encoder = TransformerEncoder(
            input_dim=user_feature_dim,
            hidden_dim=512,
            num_layers=6,
            num_heads=8
        )
        
        # Decoder: ç”Ÿæˆitemåºåˆ—
        self.item_decoder = TransformerDecoder(
            vocab_size=100000,  # è¯­ä¹‰IDè¯è¡¨å¤§å°
            hidden_dim=512,
            num_layers=6,
            num_heads=8
        )
    
    def forward(self, user_features):
        # 1. ç¼–ç ç”¨æˆ·
        user_state = self.user_encoder(user_features)
        # shape: (batch, seq_len, hidden_dim)
        
        # 2. è‡ªå›å½’ç”Ÿæˆ
        generated_ids = []
        for pos in range(K):
            logits = self.item_decoder(
                user_state, 
                generated_ids
            )  # shape: (batch, vocab_size)
            
            # é‡‡æ ·ï¼ˆè®­ç»ƒæ—¶ï¼‰æˆ–argmaxï¼ˆæ¨ç†æ—¶ï¼‰
            if training:
                item_id = sample(softmax(logits))
            else:
                item_id = argmax(logits)
            
            generated_ids.append(item_id)
        
        return generated_ids
```

**å…³é”®æŠ€æœ¯ï¼šè¯­ä¹‰ID Tokenization**

ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”Ÿæˆitem IDï¼Ÿ
```python
# é—®é¢˜1ï¼šitem IDæ˜¯ä»»æ„æ•´æ•°ï¼ˆå¦‚å•†å“ID: 2847593ï¼‰
#        æ²¡æœ‰è¯­ä¹‰ä¿¡æ¯

# é—®é¢˜2ï¼šitemæ± å¤ªå¤§ï¼ˆ100ä¸‡ï¼‰ï¼Œç›´æ¥åˆ†ç±»ä¸ç°å®

# é—®é¢˜3ï¼šæ–°itemæ— æ³•å¤„ç†ï¼ˆIDä¸åœ¨è¯è¡¨ä¸­ï¼‰
```

**è§£å†³æ–¹æ¡ˆï¼šRQ-Kmeans**
```python
# 1. ç”¨itemçš„contentç‰¹å¾ï¼ˆæ ‡é¢˜ã€å°é¢ã€æ ‡ç­¾ç­‰ï¼‰
#    è®­ç»ƒembedding
item_embeddings = ItemEmbeddingModel(item_features)
# shape: (1000000, 512)

# 2. ç”¨RQ-Kmeansé‡åŒ–ä¸ºç¦»æ•£token
tokenizer = RQKmeans(
    num_codebooks=4,  # 4å±‚ç æœ¬
    codebook_size=256  # æ¯å±‚256ä¸ªä¸­å¿ƒ
)
tokenizer.fit(item_embeddings)

# 3. æ¯ä¸ªitemç”¨4ä¸ªtokenè¡¨ç¤º
item_tokens = tokenizer.encode(item_embedding)
# ä¾‹å¦‚ï¼šitem_123 â†’ [45, 128, 67, 201]

# 4. æ€»è¯è¡¨å¤§å°ï¼š256^4 = 4Bï¼ˆè¿œå¤§äº100ä¸‡itemï¼‰
#    ä½†decoderè¾“å‡ºç»´åº¦åªæœ‰256ï¼ˆæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªå±‚çš„tokenï¼‰
```

**è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹**ï¼š
```python
# ç”ŸæˆK=10ä¸ªitemï¼Œæ¯ä¸ªitemç”¨4ä¸ªtokenè¡¨ç¤º
# æ€»å…±éœ€è¦ç”Ÿæˆ40ä¸ªtoken

user_state = encoder(user_features)
tokens = []

for pos in range(40):
    logits = decoder(user_state, tokens)  # (batch, 256)
    token = sample(softmax(logits))
    tokens.append(token)

# è§£ç ä¸ºitem ID
items = []
for i in range(0, 40, 4):
    item_tokens = tokens[i:i+4]
    item_id = tokenizer.decode(item_tokens)
    items.append(item_id)
```

### 6.3 Transformer + RL çš„èåˆ

**ä¸¤ç§èåˆæ–¹å¼å¯¹æ¯”**ï¼š

| æ–¹å¼ | è®­ç»ƒé˜¶æ®µ | æ¨ç†é˜¶æ®µ | ä¼˜åŒ–ç›®æ ‡ |
|------|---------|---------|---------|
| **æ–¹å¼1: å…ˆé¢„è®­ç»ƒå†å¾®è°ƒ** | ç›‘ç£å­¦ä¹  + RLHF | RLç­–ç•¥ | ç‚¹å‡»ç‡ â†’ é•¿æœŸç•™å­˜ |
| **æ–¹å¼2: ç«¯åˆ°ç«¯RL** | çº¯RLè®­ç»ƒ | RLç­–ç•¥ | ç›´æ¥ä¼˜åŒ–é•¿æœŸä»·å€¼ |

**OneRecé‡‡ç”¨æ–¹å¼1**ï¼š
```python
# é˜¶æ®µ1ï¼šç›‘ç£å­¦ä¹ é¢„è®­ç»ƒ
for batch in pretrain_data:
    user, gt_items = batch  # å†å²ç‚¹å‡»æ•°æ®
    
    # æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶
    pred_logits = model(user)
    loss = CrossEntropy(pred_logits, gt_items)
    loss.backward()

# é˜¶æ®µ2ï¼šRLHFå¾®è°ƒï¼ˆä½¿ç”¨ECPOï¼‰
for batch in online_data:
    user = batch
    
    # ç”Ÿæˆæ¨èåˆ—è¡¨ï¼ˆgroupï¼‰
    groups = []
    for _ in range(group_size):
        items = model.generate(user)
        reward = get_user_feedback(items)  # çœŸå®ç”¨æˆ·åé¦ˆ
        groups.append((items, reward))
    
    # ECPOæ›´æ–°
    loss = ecpo_loss(groups)
    loss.backward()
```

**èåˆçš„å¥½å¤„**ï¼š
1. **é¢„è®­ç»ƒæä¾›å¥½çš„åˆå§‹åŒ–**
   - é¿å…ä»éšæœºç­–ç•¥å¼€å§‹
   - åŠ å¿«RLè®­ç»ƒæ”¶æ•›

2. **RLä¼˜åŒ–é•¿æœŸç›®æ ‡**
   - ç›‘ç£å­¦ä¹ åªèƒ½æ¨¡ä»¿å†å²
   - RLå¯ä»¥æ¢ç´¢æ›´å¥½çš„ç­–ç•¥

3. **ç«¯åˆ°ç«¯ä¼˜åŒ–**
   - ç»Ÿä¸€å¬å›ã€æ’åºã€é‡æ’
   - å…¨å±€æœ€ä¼˜è€Œéå±€éƒ¨æœ€ä¼˜

### 6.4 å®Œæ•´æŠ€æœ¯æ ˆ

```mermaid
graph TB
    subgraph "åŸºç¡€ç†è®ºå±‚"
        A[MDPå»ºæ¨¡]
        B[è´å°”æ›¼æ–¹ç¨‹]
        C[ç­–ç•¥æ¢¯åº¦å®šç†]
    end
    
    subgraph "ç®—æ³•å±‚"
        D[PPO]
        E[GRPO]
        F[ECPO]
    end
    
    subgraph "æ¶æ„å±‚"
        G[Transformer]
        H[Encoder-Decoder]
        I[è¯­ä¹‰ID]
    end
    
    subgraph "ç³»ç»Ÿå±‚"
        J[åˆ†å¸ƒå¼è®­ç»ƒ]
        K[åœ¨çº¿æ¨ç†]
        L[A/Bæµ‹è¯•]
    end
    
    A --> D
    B --> D
    C --> D
    D --> E
    E --> F
    
    G --> H
    H --> I
    
    F --> J
    I --> J
    J --> K
    K --> L
    
    style F fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px
    style I fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style L fill:#e1bee7,stroke:#8e24aa,stroke-width:2px
```

---

## 7. å®è·µæŒ‡å—ï¼šå¦‚ä½•é€‰æ‹©ç®—æ³•ï¼Ÿ

### 7.1 å†³ç­–æ ‘

```mermaid
graph TD
    Start["æ¨èç³»ç»Ÿéœ€æ±‚"] --> Q1{"åŠ¨ä½œç©ºé—´å¤§å°?"}
    
    Q1 -->|"< 100"| V1["âœ… å¯ä»¥ä½¿ç”¨ Q-Learning"]
    Q1 -->|"100-10,000"| V2["âœ… å¯ä»¥å°è¯• DQN"]
    Q1 -->|"> 10,000"| Q2{"æ˜¯å¦éœ€è¦ç”Ÿæˆåºåˆ—?"}
    
    Q2 -->|"å¦<br/>å•æ¬¡æ¨è"| P1["æ¨è: Actor-Critic"]
    Q2 -->|"æ˜¯<br/>æ¨èåˆ—è¡¨"| Q3{"æ˜¯å¦æœ‰é¢„è®­ç»ƒæ¨¡å‹?"}
    
    Q3 -->|"å¦"| P2["æ¨è: PPOä»å¤´è®­ç»ƒ"]
    Q3 -->|"æ˜¯<br/>æœ‰ç›‘ç£æ•°æ®"| Q4{"è®¡ç®—èµ„æº?"}
    
    Q4 -->|"æœ‰é™"| P3["æ¨è: GRPO<br/>ï¼ˆæ— éœ€Criticï¼‰"]
    Q4 -->|"å……è¶³"| P4["æ¨è: ECPO + Transformer<br/>ï¼ˆç”Ÿæˆå¼æ¨èï¼‰"]
    
    style V1 fill:#ffcdd2
    style V2 fill:#ffcdd2
    style P1 fill:#fff9c4
    style P2 fill:#c5e1a5
    style P3 fill:#c5e1a5
    style P4 fill:#a5d6a7,stroke:#2e7d32,stroke-width:3px
```

### 7.2 åœºæ™¯å¯¹ç…§è¡¨

| åœºæ™¯ | itemæ± å¤§å° | åºåˆ—é•¿åº¦ | æ¨èç®—æ³• | åŸå›  |
|------|----------|---------|---------|------|
| æ–°é—»æ¨èï¼ˆå°å‹ï¼‰ | < 1000 | å•item | Q-Learning | å¯æšä¸¾ï¼Œå®ç°ç®€å• |
| ç”µå•†æ¨èï¼ˆä¸­å‹ï¼‰ | 10,000 | å•item | DQN | éœ€è¦æ³›åŒ–ï¼Œä½†å¯è¿‘ä¼¼argmax |
| è§†é¢‘æ¨èï¼ˆå¤§å‹ï¼‰ | 100,000 | Top-10 | PPO | å¤§åŠ¨ä½œç©ºé—´ï¼Œéœ€è¦ç­–ç•¥æ¢¯åº¦ |
| çŸ­è§†é¢‘æ¨èï¼ˆè¶…å¤§å‹ï¼‰ | 1,000,000+ | Top-K | GRPO | è®¡ç®—æ•ˆç‡ä¼˜å…ˆ |
| ç”Ÿæˆå¼æ¨èï¼ˆå‰æ²¿ï¼‰ | 1,000,000+ | åºåˆ—ç”Ÿæˆ | ECPO+Transformer | ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œæœ€ä½³æ•ˆæœ |

### 7.3 å®ç°å¤æ‚åº¦å¯¹æ¯”

| ç®—æ³• | å®ç°éš¾åº¦ | è®­ç»ƒæˆæœ¬ | æ¨ç†å»¶è¿Ÿ | æ•ˆæœä¸Šé™ |
|------|---------|---------|---------|---------|
| Q-Learning | â­ | ä½ | ä½ | ä½ï¼ˆå°åœºæ™¯ï¼‰ |
| DQN | â­â­ | ä¸­ | ä¸­ | ä¸­ |
| Actor-Critic | â­â­â­ | ä¸­ | ä½ | ä¸­-é«˜ |
| PPO | â­â­â­â­ | é«˜ | ä¸­ | é«˜ |
| GRPO | â­â­â­ | ä¸­ | ä¸­ | é«˜ |
| ECPO+Transformer | â­â­â­â­â­ | æé«˜ | ä¸­-é«˜ | æé«˜ |

### 7.4 å¿«é€ŸåŸå‹å»ºè®®

**é˜¶æ®µ1ï¼šéªŒè¯RLæœ‰æ•ˆæ€§**
```python
# ç”¨ç®€å•ç®—æ³•å¿«é€ŸéªŒè¯
# 1. å»ºç«‹MDPç¯å¢ƒ
# 2. ç”¨Actor-Criticæˆ–PPOè®­ç»ƒ
# 3. å¯¹æ¯”ç›‘ç£å­¦ä¹ baseline
# æ—¶é—´ï¼š1-2å‘¨
```

**é˜¶æ®µ2ï¼šä¼˜åŒ–ç®—æ³•é€‰æ‹©**
```python
# æ ¹æ®é˜¶æ®µ1ç»“æœé€‰æ‹©
if RLæ•ˆæœæ˜¾è‘—:
    if è®¡ç®—èµ„æºå……è¶³:
        å°è¯• ECPO + Transformer
    else:
        ç»§ç»­ä¼˜åŒ– PPO/GRPO
else:
    åˆ†æåŸå› ï¼š
    - å¥–åŠ±è®¾è®¡æ˜¯å¦åˆç†ï¼Ÿ
    - MDPå»ºæ¨¡æ˜¯å¦å‡†ç¡®ï¼Ÿ
    - æ˜¯å¦éœ€è¦æ›´å¤šæ¢ç´¢ï¼Ÿ
```

**é˜¶æ®µ3ï¼šå·¥ç¨‹åŒ–éƒ¨ç½²**
```python
# 1. æ¨¡å‹å‹ç¼©ï¼ˆè’¸é¦ã€é‡åŒ–ï¼‰
# 2. æ¨ç†ä¼˜åŒ–ï¼ˆTensorRTã€ONNXï¼‰
# 3. A/Bæµ‹è¯•
# 4. ç°åº¦å‘å¸ƒ
```

---

## 8. å­¦ä¹ è·¯å¾„å»ºè®®

### 8.1 ç†è®ºå­¦ä¹ è·¯å¾„

```mermaid
graph TD
    L1["ç¬¬1-3ç« <br/>RLåŸºç¡€ç†è®º"] --> L2["é˜¶æ®µ1: ç†è§£ä¸¤å¤§è·¯å¾„"]
    L2 --> L3["é˜¶æ®µ2: ç­–ç•¥æ¢¯åº¦æ–¹æ³•"]
    L3 --> L4["é˜¶æ®µ3: Transformeræ¶æ„"]
    L4 --> L5["é˜¶æ®µ4: ç”Ÿæˆå¼RL"]
    L5 --> L6["é˜¶æ®µ5: æ¨èç³»ç»Ÿåº”ç”¨"]
    
    L2 -.å¿…è¯»è®ºæ–‡.-> P1["DQN (Mnih+ 2015)"]
    L3 -.å¿…è¯»è®ºæ–‡.-> P2["PPO (Schulman+ 2017)"]
    L4 -.å¿…è¯»è®ºæ–‡.-> P3["Attention is All You Need"]
    L5 -.å¿…è¯»è®ºæ–‡.-> P4["GRPO (DeepSeek 2024)"]
    L6 -.å¿…è¯»è®ºæ–‡.-> P5["OneRec (Kuaishou 2025)"]
    
    style L1 fill:#e3f2fd
    style L6 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px
```

### 8.2 å®è·µå­¦ä¹ è·¯å¾„

**é¡¹ç›®1ï¼šQ-Learningå°å®éªŒ**
```python
# ç›®æ ‡ï¼šç†è§£Value-Basedæ–¹æ³•çš„å±€é™æ€§
# ä»»åŠ¡ï¼šç½‘æ ¼ä¸–ç•Œæ¨èï¼ˆ10Ã—10çŠ¶æ€ï¼Œ4ä¸ªitemï¼‰
# æ—¶é—´ï¼š1-2å¤©
# æ”¶è·ï¼šä½“ä¼šä¸ºä»€ä¹ˆQè¡¨åœ¨å¤§ç©ºé—´ä¸å¯è¡Œ
```

**é¡¹ç›®2ï¼šDQNè§†é¢‘æ¨è**
```python
# ç›®æ ‡ï¼šæŒæ¡æ·±åº¦Qç½‘ç»œ
# ä»»åŠ¡ï¼šå°è§„æ¨¡è§†é¢‘æ¨èï¼ˆ1000ç”¨æˆ·ï¼Œ100è§†é¢‘ï¼‰
# æ—¶é—´ï¼š1å‘¨
# æ”¶è·ï¼šç†è§£DQNçš„ä¼˜åŠ¿å’Œåœ¨æ¨èä¸­çš„ç“¶é¢ˆ
```

**é¡¹ç›®3ï¼šPPOç­–ç•¥æ¢¯åº¦**
```python
# ç›®æ ‡ï¼šæŒæ¡ç­–ç•¥æ¢¯åº¦æ–¹æ³•
# ä»»åŠ¡ï¼šæ¨¡æ‹Ÿæ¨èç¯å¢ƒï¼Œå®ç°å®Œæ•´PPO
# æ—¶é—´ï¼š2å‘¨
# æ”¶è·ï¼šç†è§£ä¸ºä»€ä¹ˆPPOæ›´é€‚åˆæ¨èç³»ç»Ÿ
```

**é¡¹ç›®4ï¼šTransformeræ¨èæ¨¡å‹**
```python
# ç›®æ ‡ï¼šç†è§£åºåˆ—å»ºæ¨¡
# ä»»åŠ¡ï¼šç”¨Transformeråšç›‘ç£å­¦ä¹ æ¨è
# æ—¶é—´ï¼š2å‘¨
# æ”¶è·ï¼šä¸ºç”Ÿæˆå¼æ¨èæ‰“åŸºç¡€
```

**é¡¹ç›®5ï¼šECPOç”Ÿæˆå¼æ¨èï¼ˆç»ˆæé¡¹ç›®ï¼‰**
```python
# ç›®æ ‡ï¼šå¤ç°OneRecæ ¸å¿ƒæ€æƒ³
# ä»»åŠ¡ï¼š
# 1. å®ç°Encoder-Decoderæ¶æ„
# 2. å®ç°è¯­ä¹‰ID Tokenization
# 3. å®ç°ECPOè®­ç»ƒ
# 4. åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¯„ä¼°
# æ—¶é—´ï¼š1-2æœˆ
# æ”¶è·ï¼šæŒæ¡ç”Ÿæˆå¼æ¨èçš„å®Œæ•´pipeline
```

### 8.3 å¿…è¯»è®ºæ–‡æ¸…å•

**åŸºç¡€ç†è®º** (1-3ç« çŸ¥è¯†)
- [ ] Sutton & Barto: *Reinforcement Learning: An Introduction* (æ•™æ)

**Value-Basedæ–¹æ³•** (ç†è§£å±€é™æ€§)
- [ ] Watkins (1989): *Q-Learning*
- [ ] Mnih+ (2013): *Playing Atari with Deep RL* (DQN)
- [ ] Mnih+ (2015): *Human-level control through deep RL* (Nature DQN)
- [ ] Hessel+ (2017): *Rainbow: Combining Improvements in Deep RL*

**Policy-Basedæ–¹æ³•** (é‡ç‚¹æŒæ¡)
- [ ] Williams (1992): *Simple Statistical Gradient-Following Algorithms* (REINFORCE)
- [ ] Konda & Tsitsiklis (2000): *Actor-Critic Algorithms*
- [ ] Schulman+ (2015): *Trust Region Policy Optimization* (TRPO)
- [ ] Schulman+ (2017): *Proximal Policy Optimization* (PPO) â­
- [ ] DeepSeek (2024): *Group Relative Policy Optimization* (GRPO) â­

**Transformerä¸ç”Ÿæˆå¼**
- [ ] Vaswani+ (2017): *Attention is All You Need* â­
- [ ] Tang & Wang (2018): *Personalized Top-N Sequential Recommendation* (SASRec)
- [ ] Sun+ (2019): *BERT4Rec: Sequential Recommendation with BERT*

**æ¨èç³»ç»Ÿåº”ç”¨** (æœ€æ–°è¿›å±•)
- [ ] Kuaishou (2025): *OneRec: One Model for Generative Recommendation* â­â­â­
- [ ] Chen+ (2019): *Top-K Off-Policy Correction for Recommender System*
- [ ] Zhao+ (2021): *Whole-Chain Recommendations* (ç”Ÿæˆå¼æ¨èæ—©æœŸå·¥ä½œ)

---

## 9. æ€»ç»“ä¸å…³é”®æ´å¯Ÿ

### 9.1 æ ¸å¿ƒè¦ç‚¹å›é¡¾

#### è¦ç‚¹ 1ï¼šç®—æ³•é€‰æ‹©ä¸æ˜¯ç†è®ºä¼˜åŠ£ï¼Œè€Œæ˜¯åœºæ™¯é€‚é…

```
ç†è®ºä¸Šï¼šValue-Based å’Œ Policy-Based éƒ½å¯ä»¥æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥

å®é™…ä¸­ï¼šæ¨èç³»ç»Ÿçš„ä¸‰å¤§ç‰¹å¾å†³å®šäº†å¿…é¡»é€‰Policy-Based
  - åŠ¨ä½œç©ºé—´çˆ†ç‚¸ï¼ˆ100ä¸‡itemæ± ï¼‰
  - è¿ç»­çŠ¶æ€ç©ºé—´ï¼ˆé«˜ç»´embeddingï¼‰
  - ç”Ÿæˆä»»åŠ¡ç‰¹æ€§ï¼ˆåºåˆ—ä¾èµ–ï¼‰
```

#### è¦ç‚¹ 2ï¼šæŠ€æœ¯æ¼”è¿›æ˜¯é˜¶æ¢¯å¼çš„

```mermaid
graph LR
    A[è´å°”æ›¼æ–¹ç¨‹<br/>1950s] --> B[Q-Learning<br/>1989]
    B --> C[DQN<br/>2013]
    C -.å¡åœ¨è¿™é‡Œ.-> D1[æ¨èç³»ç»Ÿçš„å°è¯•<br/>2015-2018]
    
    A --> E[ç­–ç•¥æ¢¯åº¦<br/>1992]
    E --> F[Actor-Critic<br/>2000s]
    F --> G[PPO<br/>2017]
    G --> H[GRPO<br/>2024]
    H --> I[ECPO<br/>2025]
    
    I --> J[ç”Ÿæˆå¼æ¨è<br/>äº§ä¸šè½åœ°]
    
    style D1 fill:#ffcdd2
    style J fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px
```

æ¯ä¸ªç®—æ³•éƒ½åœ¨è§£å†³å‰ä¸€ä»£çš„å…·ä½“é—®é¢˜ï¼š
- REINFORCE â†’ Actor-Critic: é™ä½æ–¹å·®
- Actor-Critic â†’ PPO: æå‡ç¨³å®šæ€§
- PPO â†’ GRPO: é™ä½è®¡ç®—æˆæœ¬
- GRPO â†’ ECPO: é’ˆå¯¹æ¨èåœºæ™¯ä¼˜åŒ–

#### è¦ç‚¹ 3ï¼šç†è®ºåŸºç¡€æ°¸è¿œé‡è¦

è™½ç„¶å®è·µä¸­ä¸ç”¨Q-Learning/DQNï¼Œä½†ç†è®ºä»ç„¶å…³é”®ï¼š

```
è´å°”æ›¼æ–¹ç¨‹ â†’ ä»·å€¼å‡½æ•°æ¦‚å¿µ â†’ Advantageå‡½æ•° â†’ Actor-Critic

å³ä½¿ä¸æ˜¾å¼è®¡ç®—Q(s,a)ï¼ŒAdvantageå‡½æ•° A(s,a) = Q(s,a) - V(s) çš„æ€æƒ³
ä»ç„¶æ˜¯PPO/GRPO/ECPOçš„æ ¸å¿ƒ
```

**å­¦ä¹ å¯ç¤º**ï¼š
- âœ… åŸºç¡€ç†è®ºæä¾›æ€ç»´æ¡†æ¶
- âœ… å…ˆå­¦ç®€å•ç®—æ³•å†å­¦å¤æ‚ç®—æ³•
- âœ… ç†è§£ç®—æ³•æ¼”è¿›çš„åŠ¨æœº

#### è¦ç‚¹ 4ï¼šTransformeræ˜¯èŒƒå¼è½¬å˜çš„å‚¬åŒ–å‰‚

```
ä¼ ç»Ÿæ¨èï¼šçº§è”ä¼˜åŒ–ï¼ˆå¬å› â†’ æ’åº â†’ é‡æ’ï¼‰
  é—®é¢˜ï¼šå±€éƒ¨æœ€ä¼˜

ç”Ÿæˆå¼æ¨èï¼šç«¯åˆ°ç«¯ç”Ÿæˆ
  è§£å†³ï¼šå…¨å±€ä¼˜åŒ–
  
å…³é”®ï¼šTransformeræä¾›äº†å¼ºå¤§çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›
      ä½¿å¾—"ç”Ÿæˆæ¨èåˆ—è¡¨"æˆä¸ºå¯èƒ½
```

**èåˆçš„å¨åŠ›**ï¼š
```
Transformerï¼ˆæ¶æ„ï¼‰ + ECPOï¼ˆä¼˜åŒ–ç®—æ³•ï¼‰ = OneRecï¼ˆäº§ä¸šåº”ç”¨ï¼‰
```

### 9.2 ä»01_basicsåˆ°OneRecçš„å®Œæ•´æ•…äº‹

**ç¬¬1ç« **ï¼šæˆ‘ä»¬å­¦ä¹ äº†RLçš„åŸºæœ¬æ¦‚å¿µ
- Agentã€Environmentã€Rewardã€Policy
- å»ºç«‹äº†"é€šè¿‡äº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥"çš„æ€ç»´

**ç¬¬2ç« **ï¼šæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•å°†æ¨èå»ºæ¨¡ä¸ºMDP
- çŠ¶æ€ = ç”¨æˆ·ç‰¹å¾
- åŠ¨ä½œ = æ¨èitem
- å¥–åŠ± = ç”¨æˆ·åé¦ˆ

**ç¬¬3ç« **ï¼šæˆ‘ä»¬å­¦ä¹ äº†è´å°”æ›¼æ–¹ç¨‹
- ä»·å€¼å‡½æ•°çš„é€’å½’å…³ç³»
- ä¸ºQ-Learningå’ŒPolicy Gradientæä¾›ç†è®ºåŸºç¡€

**ç¬¬4ç« **ï¼šæˆ‘ä»¬ç†è§£äº†ä¸ºä»€ä¹ˆç°ä»£æ¨èé€‰æ‹©Policy-Based
- Value-Basedåœ¨æ¨èç³»ç»Ÿé‡åˆ°ç“¶é¢ˆ
- Policy-Based + Transformer æˆä¸ºç”Ÿæˆå¼æ¨èçš„åŸºç¡€
- ECPOæ˜¯é’ˆå¯¹æ¨èåœºæ™¯çš„æœ€æ–°ä¼˜åŒ–

### 9.3 å…³é”®æ´å¯Ÿ

**æ´å¯Ÿ 1**ï¼šä¸æ˜¯"è·³è¿‡"ï¼Œè€Œæ˜¯"é€‰æ‹©"
```
ä¸æ˜¯è¯´Q-Learning/DQNä¸å¥½ï¼Œè€Œæ˜¯ä¸é€‚åˆå¤§è§„æ¨¡æ¨è

å°è§„æ¨¡åœºæ™¯ï¼ˆitemæ±  < 1000ï¼‰ï¼š
  Q-Learning/DQNä»ç„¶æœ‰æ•ˆä¸”å®ç°ç®€å•

å¤§è§„æ¨¡åœºæ™¯ï¼ˆitemæ±  > 10ä¸‡ï¼‰ï¼š
  å¿…é¡»ä½¿ç”¨Policy-Basedæ–¹æ³•
```

**æ´å¯Ÿ 2**ï¼šç†è®ºä¸å·¥ç¨‹çš„å¹³è¡¡
```
çº¯ç†è®ºï¼šè´å°”æ›¼æ–¹ç¨‹ä¿è¯Q-Learningæ”¶æ•›åˆ°æœ€ä¼˜

çº¯å·¥ç¨‹ï¼šargmaxæ“ä½œåœ¨100ä¸‡itemæ± ä¸Šä¸å¯è¡Œ

å¹³è¡¡ï¼šç†è§£ç†è®ºâ†’è¯†åˆ«é™åˆ¶â†’é€‰æ‹©åˆé€‚ç®—æ³•
```

**æ´å¯Ÿ 3**ï¼šæŠ€æœ¯èåˆåˆ›é€ æ–°èŒƒå¼
```
å•ä¸€æŠ€æœ¯ï¼š
  - RL aloneï¼šéš¾ä»¥å¤„ç†é«˜ç»´çŠ¶æ€å’Œå¤§åŠ¨ä½œç©ºé—´
  - Transformer aloneï¼šç›‘ç£å­¦ä¹ åªèƒ½æ¨¡ä»¿å†å²

èåˆåˆ›æ–°ï¼š
  - Transformerï¼ˆå¼ºå¤§åºåˆ—å»ºæ¨¡ï¼‰ + RLï¼ˆé•¿æœŸä¼˜åŒ–ï¼‰
  - = ç”Ÿæˆå¼æ¨èï¼ˆç«¯åˆ°ç«¯ã€é•¿æœŸä»·å€¼ä¼˜åŒ–ï¼‰
```

### 9.4 ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®

**å¦‚æœä½ æƒ³æ·±å…¥ç†è®º**ï¼š
1. é˜…è¯»Sutton & Bartoçš„RLæ•™æ
2. æ¨å¯¼ç­–ç•¥æ¢¯åº¦å®šç†
3. ç†è§£PPOçš„ç†è®ºä¿è¯

**å¦‚æœä½ æƒ³åŠ¨æ‰‹å®è·µ**ï¼š
1. å®ç°Q-Learningç†è§£å±€é™
2. å®ç°PPOæŒæ¡ç­–ç•¥æ¢¯åº¦
3. å¤ç°OneRecçš„æ ¸å¿ƒæ€æƒ³

**å¦‚æœä½ æƒ³è§£å†³å®é™…é—®é¢˜**ï¼š
1. åˆ†æä½ çš„æ¨èåœºæ™¯ç‰¹ç‚¹
2. ç”¨å†³ç­–æ ‘é€‰æ‹©åˆé€‚ç®—æ³•
3. ä»ç®€å•baselineå¼€å§‹è¿­ä»£

---

## 10. é™„å½•ï¼šå®Œæ•´æ¼”è¿›å›¾è°±

### 10.1 åŸºç¡€ç†è®ºåˆ°ç°ä»£åº”ç”¨

```mermaid
graph TB
    subgraph Theory["åŸºç¡€ç†è®ºå±‚ (01-03ç« )"]
        A["RLæ ¸å¿ƒæ¦‚å¿µ<br/>Agent-Environment-Reward"]
        B["MDP<br/>âŸ¨S,A,P,R,Î³âŸ©"]
        C["ä»·å€¼å‡½æ•°<br/>V(s) å’Œ Q(s,a)"]
        D["è´å°”æ›¼æ–¹ç¨‹<br/>ä»·å€¼é€’å½’åˆ†è§£"]
        
        A --> B
        B --> C
        B --> D
    end
    
    subgraph Methods["ç»å…¸å­¦ä¹ æ–¹æ³•"]
        E["åŠ¨æ€è§„åˆ’ DP<br/>éœ€è¦å®Œæ•´æ¨¡å‹"]
        F["è’™ç‰¹å¡æ´› MC<br/>å®Œæ•´å›åˆé‡‡æ ·"]
        G["æ—¶åºå·®åˆ† TD<br/>åœ¨çº¿+è‡ªä¸¾"]
        
        D --> E
        D --> F
        D --> G
    end
    
    subgraph ValueBased["Value-Based è·¯å¾„ (æ¨èç³»ç»Ÿå—é˜»)"]
        H["Q-Learning<br/>Off-Policy"]
        I["SARSA<br/>On-Policy"]
        J["DQN 2013<br/>æ·±åº¦Qç½‘ç»œ"]
        K["Rainbow DQN<br/>å¤šé¡¹æ”¹è¿›èåˆ"]
        
        G --> H
        G --> I
        H --> J
        J --> K
    end
    
    subgraph PolicyBased["Policy-Based è·¯å¾„ (æ¨èç³»ç»Ÿä¸»æµ)"]
        L["REINFORCE<br/>ç­–ç•¥æ¢¯åº¦åŸºç¡€"]
        M["Actor-Critic<br/>ä»·å€¼+ç­–ç•¥"]
        N["TRPO 2015<br/>Trust Region"]
        O["PPO 2017<br/>Clipped Objective"]
        P["GRPO 2024<br/>Group Relative"]
        
        C --> L
        L --> M
        M --> N
        N --> O
        O --> P
    end
    
    subgraph Modern["æ·±åº¦å­¦ä¹ æŠ€æœ¯"]
        Q["Transformer 2017<br/>Self-Attention"]
        R["Encoder-Decoder<br/>åºåˆ—åˆ°åºåˆ—"]
        
        Q --> R
    end
    
    subgraph RL_Advanced["é«˜çº§ç­–ç•¥ä¼˜åŒ–"]
        S["ECPO 2025<br/>Enhanced Clipping"]
        
        P --> S
        O --> S
    end
    
    subgraph OneRec["ç”Ÿæˆå¼æ¨èåº”ç”¨"]
        T["OneRec<br/>Transformer+RL"]
        
        R --> T
        S --> T
    end
    
    K -.åœ¨æ¨èåœºæ™¯å¤±æ•ˆ.-> T
    
    classDef traditional fill:#e1f5ff,stroke:#01579b,stroke-width:2px
    classDef skipped fill:#ffebee,stroke:#c62828,stroke-width:2px,stroke-dasharray:5 5
    classDef modern fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef rl_advanced fill:#ffecb3,stroke:#f57c00,stroke-width:2px
    classDef onerec fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px
    
    class A,B,C,D,E,F,G traditional
    class H,I,J,K skipped
    class L,M,N,O,P modern
    class Q,R modern
    class S rl_advanced
    class T onerec
```

### 10.2 æ—¶é—´çº¿è§†å›¾

```mermaid
timeline
    title å¼ºåŒ–å­¦ä¹ åœ¨æ¨èç³»ç»Ÿçš„åº”ç”¨æ¼”è¿›
    
    1950s : è´å°”æ›¼æ–¹ç¨‹ : åŠ¨æ€è§„åˆ’ç†è®ºåŸºç¡€
    1989 : Q-Learning : Watkinsåšå£«è®ºæ–‡
    1992 : REINFORCE : Williamsç­–ç•¥æ¢¯åº¦
    2000 : Actor-Critic : Konda & Tsitsiklis
    2013 : DQNçªç ´ : Atariæ¸¸æˆ : Mnih et al.
    2015 : TRPO : ä¿¡ä»»åŸŸä¼˜åŒ– : Schulman et al.
    2016 : A3C : å¼‚æ­¥ä¼˜åŠ¿Actor-Critic
    2017 : PPO : OpenAIä¸»æ¨ç®—æ³•
    2017 : Transformer : Attention is All You Need
    2018 : BERT/GPT : é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
    2019 : BERT4Rec : Transformerç”¨äºæ¨è
    2022 : ChatGPT : RLHFè¯æ˜ç­–ç•¥æ¢¯åº¦å¨åŠ›
    2024 : GRPO : DeepSeekä¼˜åŒ–å¤§æ¨¡å‹å¾®è°ƒ
    2025 : OneRec : å¿«æ‰‹ç”Ÿæˆå¼æ¨èè½åœ°
```

### 10.3 ç®—æ³•å¯¹æ¯”çŸ©é˜µ

| ç»´åº¦ | Q-Learning | DQN | Actor-Critic | PPO | GRPO | ECPO |
|------|-----------|-----|--------------|-----|------|------|
| **æå‡ºå¹´ä»½** | 1989 | 2013 | 2000 | 2017 | 2024 | 2025 |
| **è·¯å¾„** | Value | Value | Hybrid | Policy | Policy | Policy |
| **åŠ¨ä½œç©ºé—´** | å°ç¦»æ•£ | ä¸­ç¦»æ•£ | ä¸­-å¤§ | å¤§ | å¤§ | æå¤§ |
| **çŠ¶æ€ç©ºé—´** | å°ç¦»æ•£ | é«˜ç»´è¿ç»­ | é«˜ç»´è¿ç»­ | é«˜ç»´è¿ç»­ | é«˜ç»´è¿ç»­ | é«˜ç»´è¿ç»­ |
| **ç”Ÿæˆä»»åŠ¡** | âŒ | âŒ | âš ï¸ | âœ… | âœ… | âœ…âœ… |
| **æ ·æœ¬æ•ˆç‡** | é«˜ | ä¸­ | ä¸­ | ä¸­-ä½ | ä¸­ | ä¸­ |
| **è®­ç»ƒç¨³å®šæ€§** | é«˜ | ä¸­ | ä¸­ | é«˜ | é«˜ | é«˜ |
| **è®¡ç®—æˆæœ¬** | ä½ | ä¸­ | ä¸­ | é«˜ | ä¸­ | æé«˜ |
| **æ¨èç³»ç»Ÿ** | å°è§„æ¨¡ | ä¸­è§„æ¨¡ | ä¸­è§„æ¨¡ | å¤§è§„æ¨¡ | å¤§è§„æ¨¡ | è¶…å¤§è§„æ¨¡ |
| **å®ç°éš¾åº¦** | â­ | â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |

---

**æ–‡æ¡£å®Œæˆäº 2026-02-16**  
**å›¾è¡¨å·¥å…·ï¼šMermaid**  
**å‚è€ƒèµ„æ–™ï¼šSutton & Barto RLæ•™æ, PPOè®ºæ–‡, OneRecæŠ€æœ¯æŠ¥å‘Š**

---

## è¿›ä¸€æ­¥å­¦ä¹ èµ„æº

**åœ¨çº¿è¯¾ç¨‹**ï¼š
- David Silver's RL Course (DeepMind)
- CS285: Deep RL (UC Berkeley)
- Spinning Up in Deep RL (OpenAI)

**ä»£ç å®ç°**ï¼š
- OpenAI Baselines (PPO/DQNç­‰æ ‡å‡†å®ç°)
- Stable-Baselines3 (æ˜“ç”¨çš„RLåº“)
- RLlib (Rayçš„åˆ†å¸ƒå¼RLæ¡†æ¶)

**æ¨èç³»ç»Ÿ**ï¼š
- RecBole (æ¨èç³»ç»Ÿå·¥å…·åŒ…)
- ReChorus (åºåˆ—æ¨èbaseline)

**è®ºæ–‡é˜…è¯»é¡ºåº**ï¼š
1. å…ˆè¯»PPOç†è§£ç­–ç•¥æ¢¯åº¦
2. å†è¯»GRPOç†è§£æœ€æ–°ä¼˜åŒ–
3. æœ€åè¯»OneRecç†è§£äº§ä¸šåº”ç”¨
